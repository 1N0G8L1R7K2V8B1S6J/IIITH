{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"id":"ILC1U1bmY79j","execution":{"iopub.status.busy":"2024-03-21T13:34:39.591502Z","iopub.execute_input":"2024-03-21T13:34:39.591899Z","iopub.status.idle":"2024-03-21T13:34:40.582553Z","shell.execute_reply.started":"2024-03-21T13:34:39.591859Z","shell.execute_reply":"2024-03-21T13:34:40.581789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sentiment Classification Using RNNs\n","\n","* Given the IMDB Movie Review Dataset, create an RNN model that predicts whether the given review is negative or positive.\n","* You need to create your Dataset, Dataloader and Model. Keep your code modular and avoid hardcoding any parameter. This will allow you to experiment more easily.\n","* Plot graphs for loss and accuracy for each epoch of a training loop. Try using wandb for logging training and validation losses, accuracies (especially for hyperparameter tuning).\n","* Use tqdm to keep track of the status of the training loop for an epoch."],"metadata":{"id":"IdzeYa_AqAuk"}},{"cell_type":"markdown","source":["### 1. RNN Model\n","#### 1.1 Build a Dataset from the IMDB Movie Review Dataset by taking reviews with word count from 100 to 500. Perform text processing on the movie reviews and create a word to index mapping for representing any review as a list of numbers.\n"],"metadata":{"id":"Q6NN7DndxvFt"}},{"cell_type":"code","source":["# !pip install datasets torchmetrics"],"metadata":{"id":"DpR8af5l1FZK","execution":{"iopub.status.busy":"2024-03-21T13:34:40.584090Z","iopub.execute_input":"2024-03-21T13:34:40.584460Z","iopub.status.idle":"2024-03-21T13:34:40.588485Z","shell.execute_reply.started":"2024-03-21T13:34:40.584436Z","shell.execute_reply":"2024-03-21T13:34:40.587599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# import matplotlib.pyplot as plt\n","\n","# from datasets import load_dataset\n","# import re\n","# from nltk.tokenize import RegexpTokenizer\n","# from nltk.stem import WordNetLemmatizer\n","\n","# import torch\n","# from torch import nn\n","# import torch.optim as optim\n","# from torch.utils.data import Dataset, DataLoader\n","# from torch.nn.utils.rnn import pad_sequence\n","# from torchmetrics import Accuracy\n","\n","# from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn as nn\n","import torch.optim as optim\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","import nltk, subprocess\n","import random\n","\n","\n","from sklearn.datasets import load_files\n","\n","# Load the IMDB dataset\n","!pip install datasets\n","from datasets import load_dataset\n","# load the IMDB review dataset. You can take the dataset from Huggingface\n","imdb_dataset = load_dataset(\"imdb\")\n"],"metadata":{"id":"DKFUUtZV09Ap","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["66b9935351f24103bb9ea3c2d1cf56e8","3c5ba101d8084ac7be5b4fea8ac743a5","bd3639b9fc764ec780180e51af0eedda","","6f25d680d10a4b629f678ea18b4efa77"]},"executionInfo":{"status":"ok","timestamp":1710993051933,"user_tz":-330,"elapsed":40554,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"a0750cfc-24b7-4abd-872e-3342ef83ff9c","execution":{"iopub.status.busy":"2024-03-21T13:34:40.589517Z","iopub.execute_input":"2024-03-21T13:34:40.589781Z","iopub.status.idle":"2024-03-21T13:36:01.590747Z","shell.execute_reply.started":"2024-03-21T13:34:40.589737Z","shell.execute_reply":"2024-03-21T13:36:01.589676Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.3.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b9935351f24103bb9ea3c2d1cf56e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c5ba101d8084ac7be5b4fea8ac743a5"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd3639b9fc764ec780180e51af0eedda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f25d680d10a4b629f678ea18b4efa77"}},"metadata":{}}]},{"cell_type":"code","source":["SEED = 1234\n","\n","# set seed for all possible random functions to ensure reproducibility\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic=True"],"metadata":{"id":"DbVR4diJ4BxO","execution":{"iopub.status.busy":"2024-03-21T13:36:01.593202Z","iopub.execute_input":"2024-03-21T13:36:01.593728Z","iopub.status.idle":"2024-03-21T13:36:01.602688Z","shell.execute_reply.started":"2024-03-21T13:36:01.593698Z","shell.execute_reply":"2024-03-21T13:36:01.601626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"],"metadata":{"id":"b-6qkjYjbhhs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710993051934,"user_tz":-330,"elapsed":10,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"5cd010f4-e0ea-47ac-bba4-a69f9ce5f112","execution":{"iopub.status.busy":"2024-03-21T13:36:01.603954Z","iopub.execute_input":"2024-03-21T13:36:01.604344Z","iopub.status.idle":"2024-03-21T13:36:01.640102Z","shell.execute_reply.started":"2024-03-21T13:36:01.604304Z","shell.execute_reply":"2024-03-21T13:36:01.639104Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"code","source":[],"metadata":{"id":"O39R0KOF1Iw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the train set into train and validation in 80-20 split. Use the labels\n","# to ensure that the ratio of the samples from each label is maintained\n","# Filter reviews by word count between 100 and 500\n","def filter_reviews_by_word_count(dataset, min_word_count=100, max_word_count=500):\n","    filtered_data = []\n","    for example in dataset:\n","        text = example[\"text\"]\n","        word_count = len(text.split())\n","        if min_word_count <= word_count <= max_word_count:\n","            filtered_data.append(example)\n","    return filtered_data\n","\n","# Filter reviews by word count\n","train_data = filter_reviews_by_word_count(imdb_dataset[\"train\"])\n","test_filtered = filter_reviews_by_word_count(imdb_dataset[\"test\"])\n","\n","# Determine the size of the validation set\n","val_size = int(0.2 * len(test_filtered))\n","test_size = len(test_filtered) - val_size\n","\n","# Randomly select reviews for validation\n","val_indices = random.sample(range(len(test_filtered)), val_size)\n","val_data = [test_filtered[i] for i in val_indices]\n","\n","# Assign the remaining reviews to test_data\n","test_data = [review for i, review in enumerate(test_filtered) if i not in val_indices]\n","\n","# Print the sizes of val_data and test_data\n","print(\"Size of train_data:\", len(train_data))\n","print(\"Size of val_data:\", len(val_data))\n","print(\"Size of test_data:\", len(test_data))"],"metadata":{"id":"9a4eux6zDy7t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710993064738,"user_tz":-330,"elapsed":4266,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"82d134b6-3fcf-4238-ef05-6b8105af14fc","execution":{"iopub.status.busy":"2024-03-21T13:36:01.641489Z","iopub.execute_input":"2024-03-21T13:36:01.641818Z","iopub.status.idle":"2024-03-21T13:36:06.301975Z","shell.execute_reply.started":"2024-03-21T13:36:01.641788Z","shell.execute_reply":"2024-03-21T13:36:06.300943Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Size of train_data: 20056\nSize of val_data: 4004\nSize of test_data: 16020\n","output_type":"stream"}]},{"cell_type":"code","source":["# Shuffle the data\n","np.random.shuffle(train_data)\n","np.random.shuffle(test_data)\n","# np.random.shuffle(val_data)\n","\n","# Optionally, limit the size of the datasets\n","# test_data = test_data[:5000]      # Limiting to 5,000 samples for faster compute\n","\n","print(\"Number of samples in train_data:\", len(train_data))\n","print(\"Number of samples in test_data:\", len(test_data))\n","print(\"Number of samples in val_data:\", len(val_data))\n","\n","\n","# Function to find the longest sentence\n","def find_longest_sentence(dataset):\n","    longest_sentence = \"\"\n","    max_length = 0\n","    for example in dataset:\n","        text = example[\"text\"]\n","        if len(text.split()) > max_length:\n","            longest_sentence = text\n","            max_length = len(text.split())\n","    return longest_sentence, max_length  # Return both the longest sentence and its length\n","\n","# Find longest sentence in each dataset\n","longest_train_sentence, train_max_length = find_longest_sentence(train_data)\n","longest_test_sentence, test_max_length = find_longest_sentence(test_data)\n","longest_val_sentence, val_max_length = find_longest_sentence(val_data)\n","\n","# Print the longest sentence and its length\n","print(\"Longest sentence in train_data:\")\n","print(\"Number of words:\", train_max_length)\n","\n","print(\"\\nLongest sentence in test_data:\")\n","print(\"Number of words:\", test_max_length)\n","\n","print(\"\\nLongest sentence in val_data:\")\n","print(\"Number of words:\", val_max_length)\n"],"metadata":{"id":"wdwm6t8kVni1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710993065336,"user_tz":-330,"elapsed":600,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"14cf3bb2-a49f-477a-9d99-44ee13dd6b56","execution":{"iopub.status.busy":"2024-03-21T13:36:06.303045Z","iopub.execute_input":"2024-03-21T13:36:06.303375Z","iopub.status.idle":"2024-03-21T13:36:06.938955Z","shell.execute_reply.started":"2024-03-21T13:36:06.303346Z","shell.execute_reply":"2024-03-21T13:36:06.937697Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Number of samples in train_data: 20056\nNumber of samples in test_data: 16020\nNumber of samples in val_data: 4004\nLongest sentence in train_data:\nNumber of words: 500\n\nLongest sentence in test_data:\nNumber of words: 500\n\nLongest sentence in val_data:\nNumber of words: 500\n","output_type":"stream"}]},{"cell_type":"code","source":["# def clean(text, tokenizer):\n","#   # Perform text preprocessing:\n","#   # 1. Removing numbers OR replace them with \"num\" token\n","#   # 2. Convert all characters to lowercase.\n","#   # 3. Tokenize the sentence into words\n","#   # You can use RegexpTokenizer from NLTK.\n","\n","#   # You will experiment with stemming/lemmatization down the line\n","#   # so you can skip that for now\n","\n","#   return text"],"metadata":{"id":"hJvJGlDo3rHa","execution":{"iopub.status.busy":"2024-03-21T13:36:06.940140Z","iopub.execute_input":"2024-03-21T13:36:06.940450Z","iopub.status.idle":"2024-03-21T13:36:06.945343Z","shell.execute_reply.started":"2024-03-21T13:36:06.940424Z","shell.execute_reply":"2024-03-21T13:36:06.944137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clean(\"This IS 1 example sentence\", RegexpTokenizer(r'\\w+'))"],"metadata":{"id":"aB5nrRbnaUbt","execution":{"iopub.status.busy":"2024-03-21T13:36:06.947018Z","iopub.execute_input":"2024-03-21T13:36:06.947800Z","iopub.status.idle":"2024-03-21T13:36:06.955896Z","shell.execute_reply.started":"2024-03-21T13:36:06.947760Z","shell.execute_reply":"2024-03-21T13:36:06.954807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","try:\n","    nltk.data.find('punkt.zip')\n","except:\n","    nltk.download('punkt', download_dir='/kaggle/working/')\n","    subprocess.run(\"unzip /usr/share/nltk_data/corpora/punkt.zip -d /usr/share/nltk_data/corpora/\".split())\n","    nltk.data.path.append('/kaggle/input')\n"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T13:36:06.961499Z","iopub.execute_input":"2024-03-21T13:36:06.961919Z","iopub.status.idle":"2024-03-21T13:36:07.590638Z","shell.execute_reply.started":"2024-03-21T13:36:06.961882Z","shell.execute_reply":"2024-03-21T13:36:07.589609Z"},"trusted":true,"id":"DdY-tg-fRHrV","outputId":"a7de2963-d1ff-47eb-c155-118e2828b8ee"},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt to /kaggle/working/...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"},{"name":"stderr","text":"unzip:  cannot find or open /usr/share/nltk_data/corpora/punkt.zip, /usr/share/nltk_data/corpora/punkt.zip.zip or /usr/share/nltk_data/corpora/punkt.zip.ZIP.\n","output_type":"stream"}]},{"cell_type":"code","source":["nltk.download('stopwords')\n","try:\n","    nltk.data.find('stopwords.zip')\n","except:\n","    nltk.download('stopwords', download_dir='/kaggle/working/')\n","    subprocess.run(\"unzip /usr/share/nltk_data/corpora/stopwords.zip -d /usr/share/nltk_data/corpora/\".split())\n","    nltk.data.path.append('/kaggle/input')"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T13:36:07.592032Z","iopub.execute_input":"2024-03-21T13:36:07.592379Z","iopub.status.idle":"2024-03-21T13:36:07.690291Z","shell.execute_reply.started":"2024-03-21T13:36:07.592350Z","shell.execute_reply":"2024-03-21T13:36:07.689381Z"},"trusted":true,"id":"4kH8Yn4SRHrV","outputId":"ebcf3d9f-e49e-4336-b3b2-211fda3fe63b"},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package stopwords to /kaggle/working/...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nArchive:  /usr/share/nltk_data/corpora/stopwords.zip\n","output_type":"stream"},{"name":"stderr","text":"replace /usr/share/nltk_data/corpora/stopwords/dutch? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n","output_type":"stream"}]},{"cell_type":"code","source":["nltk.download('wordnet')\n","try:\n","    nltk.data.find('wordnet.zip')\n","except:\n","    nltk.download('wordnet', download_dir='/kaggle/working/')\n","    subprocess.run(\"unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\".split())\n","    nltk.data.path.append('/kaggle/input')"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-21T13:36:07.691654Z","iopub.execute_input":"2024-03-21T13:36:07.692039Z","iopub.status.idle":"2024-03-21T13:36:08.255098Z","shell.execute_reply.started":"2024-03-21T13:36:07.692011Z","shell.execute_reply":"2024-03-21T13:36:08.254026Z"},"trusted":true,"id":"13uuzeuXRHrV","outputId":"5937a75f-f854-4a4f-c553-e04c83af32f7"},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":["# # Text preprocessing\n","# def preprocess_text(text):\n","#     text = text.lower()  # Convert to lowercase\n","#     text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n","#     text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n","#     return text\n","\n","# # Tokenization and lemmatization\n","# lemmatizer = WordNetLemmatizer()\n","# stop_words = set(stopwords.words('english'))\n","\n","# def tokenize_and_lemmatize(text):\n","#     tokens = word_tokenize(text)\n","#     tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n","#     return tokens"],"metadata":{"id":"mJwTzbbZWMbc","execution":{"iopub.status.busy":"2024-03-21T13:36:08.256317Z","iopub.execute_input":"2024-03-21T13:36:08.256612Z","iopub.status.idle":"2024-03-21T13:36:08.261798Z","shell.execute_reply.started":"2024-03-21T13:36:08.256587Z","shell.execute_reply":"2024-03-21T13:36:08.260408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # create a word to index dictionary so that each word in the training set\n","# # has a number associated with it. This allows to represent each sentence\n","# # as a series of numbers. Start the index with 1 instead of 0. The number\n","# # 0 will be used to denote padding, so that each sentence can have the\n","# # same length.\n","# # Keep track of the index since it will be used for representing new words\n","# # that were not part of the training vocabulary.\n","# # Also, make sure to not create dictionary on sentences with word count\n","# # not within the range\n","\n","# def get_word2idx(corpus):\n","#   idx = 1\n","#   for sentence in tqdm(corpus, total=len(corpus), desc=\"Creating word2idx\"):\n","#     # process sentence\n","#     sentence = clean(sentence, tokenizer)\n","\n","#     # drop sentences greater than maxlen or less than minlen\n","\n","#     # for each word in sentence, check for entry in word2idx\n","\n","#   return idx, word2idx"],"metadata":{"id":"WEDLk2ZXUwVU","execution":{"iopub.status.busy":"2024-03-21T13:36:08.263180Z","iopub.execute_input":"2024-03-21T13:36:08.263498Z","iopub.status.idle":"2024-03-21T13:36:08.273806Z","shell.execute_reply.started":"2024-03-21T13:36:08.263466Z","shell.execute_reply":"2024-03-21T13:36:08.272800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Text preprocessing\n","def preprocess_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n","    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n","    return text\n","\n","def tokenize_and_lemmatize(text):\n","    tokens = word_tokenize(text)\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n","    return tokens\n","\n","# Build vocabulary\n","def build_vocab(data):\n","    word_to_idx = {}\n","    idx_to_word = {}\n","    for review in data:\n","        tokens = tokenize_and_lemmatize(preprocess_text(review['text']))\n","        for token in tokens:\n","            if token not in word_to_idx:\n","                idx = len(word_to_idx)\n","                word_to_idx[token] = idx\n","                idx_to_word[idx] = token\n","    return word_to_idx, idx_to_word\n","\n","# Tokenization and lemmatization\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","# Modify the code according to train, test, val datasets\n","word_to_idx_train, _ = build_vocab(train_data)\n","\n","# Find a unique index for UNK\n","max_idx = max(word_to_idx_train.values()) if word_to_idx_train else -1\n","unk_idx = max_idx + 1\n","\n","# Add UNK to word_to_idx_train\n","word_to_idx_train['UNK'] = unk_idx"],"metadata":{"id":"eh04W3rSWDBV","execution":{"iopub.status.busy":"2024-03-21T13:36:08.274957Z","iopub.execute_input":"2024-03-21T13:36:08.275304Z","iopub.status.idle":"2024-03-21T13:37:06.053672Z","shell.execute_reply.started":"2024-03-21T13:36:08.275259Z","shell.execute_reply":"2024-03-21T13:37:06.052820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1.2 Create Dataloaders for the train, test and validation datasets with appropriate batch sizes.\n"],"metadata":{"id":"PDtdun61-H4H"}},{"cell_type":"code","source":["# # Build a Dataset object to store each sentence as a tensor of numbers\n","# # along with the label. Make sure to add padding so that the tensor\n","# # for each sentence is of the same length. This will allow us to train\n","# # the model in batches.\n","\n","# class IMDBDataset(Dataset):\n","#   def __init__(self, dataset, split : str, minlen : int = 100, maxlen : int = 500):\n","#     self.count = 0 # total sentences you finally pick\n","\n","#     # count total number of lines\n","#     len = len(dataset[split])\n","\n","#     input_data = []\n","#     target_data = []\n","\n","#     for idx, sentence in tqdm(enumerate(corpus), total=len, desc=f\"Transforming input text [{split}]\"):\n","#       # process sentence\n","\n","#       # drop sentences greater than maxlen or less than minlen\n","\n","#       # replace words with their index\n","\n","\n","#       self.count += 1\n","\n","#     # pad the sentences upto maxlen\n","#     self.inputs = pad_sequence(input_data, batch_first = True)\n","#     self.targets = torch.tensor(target_data)\n","\n","#   def __len__(self) -> int:\n","#     return self.count\n","\n","#   def __getitem__(self, index : int):\n","#     return self.inputs[index], self.targets[index]"],"metadata":{"id":"M9g3b_S82ODM","execution":{"iopub.status.busy":"2024-03-21T13:37:06.055038Z","iopub.execute_input":"2024-03-21T13:37:06.055906Z","iopub.status.idle":"2024-03-21T13:37:06.061945Z","shell.execute_reply.started":"2024-03-21T13:37:06.055868Z","shell.execute_reply":"2024-03-21T13:37:06.060825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # create the train dataset using the word2idx dictionary built using the train set\n","# train_ds = IMDBDataset(imdb_dataset, \"train\",minlen = 100, maxlen = 500)\n","# # create the validation and test dataset using the word2idx dictionary built using the train set\n","\n"],"metadata":{"id":"CGQ-5qXKG0Ag","execution":{"iopub.status.busy":"2024-03-21T13:37:06.063429Z","iopub.execute_input":"2024-03-21T13:37:06.063766Z","iopub.status.idle":"2024-03-21T13:37:06.079111Z","shell.execute_reply.started":"2024-03-21T13:37:06.063727Z","shell.execute_reply":"2024-03-21T13:37:06.078176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# len(train_ds), len(val_ds), len(test_ds)"],"metadata":{"id":"sgVEZqCNfIcC","execution":{"iopub.status.busy":"2024-03-21T13:37:06.080197Z","iopub.execute_input":"2024-03-21T13:37:06.080482Z","iopub.status.idle":"2024-03-21T13:37:06.089452Z","shell.execute_reply.started":"2024-03-21T13:37:06.080460Z","shell.execute_reply":"2024-03-21T13:37:06.088512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # create dataloaders using the dataset\n","# params = {\n","#     'batch_size':32,\n","#     'shuffle': True,\n","#     'num_workers': 2\n","# }\n","\n","# train_dataloader = DataLoader(train_ds, **params)\n","# test_dataloader = DataLoader(val_ds, **params)\n","# test_dataloader = DataLoader(test_ds, **params)"],"metadata":{"id":"FFQcp_oLTtDi","execution":{"iopub.status.busy":"2024-03-21T13:37:06.090696Z","iopub.execute_input":"2024-03-21T13:37:06.091101Z","iopub.status.idle":"2024-03-21T13:37:06.099888Z","shell.execute_reply.started":"2024-03-21T13:37:06.091046Z","shell.execute_reply":"2024-03-21T13:37:06.098819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IMDBDataset(Dataset):\n","    def __init__(self, data, word_to_idx, max_length=500):\n","        self.data = data\n","        self.word_to_idx = word_to_idx\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        review = self.data[index]['text']\n","        label = int(self.data[index]['label'])  # Convert label to integer\n","        tokens = tokenize_and_lemmatize(preprocess_text(review))\n","        indexed_tokens = [self.word_to_idx.get(token, max_idx + 1) for token in tokens]  # Use .get() to handle missing keys\n","        indexed_tokens = indexed_tokens[:self.max_length]  # Trim to max length\n","        padded_tokens = indexed_tokens + [0] * (self.max_length - len(indexed_tokens))  # Pad sequence\n","        return torch.tensor(padded_tokens), torch.tensor(label)\n"],"metadata":{"id":"_NNzMu8QWi3_","execution":{"iopub.status.busy":"2024-03-21T13:37:06.101112Z","iopub.execute_input":"2024-03-21T13:37:06.101426Z","iopub.status.idle":"2024-03-21T13:37:06.110958Z","shell.execute_reply.started":"2024-03-21T13:37:06.101403Z","shell.execute_reply":"2024-03-21T13:37:06.109989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create DataLoader\n","def collate_fn(batch):\n","    inputs = [item[0] for item in batch]\n","    labels = [item[1] for item in batch]\n","    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n","    labels = torch.tensor(labels)\n","    return padded_inputs, labels\n"],"metadata":{"id":"ftZI0U39Wk-5","execution":{"iopub.status.busy":"2024-03-21T13:37:06.112306Z","iopub.execute_input":"2024-03-21T13:37:06.112612Z","iopub.status.idle":"2024-03-21T13:37:06.127144Z","shell.execute_reply.started":"2024-03-21T13:37:06.112586Z","shell.execute_reply":"2024-03-21T13:37:06.126032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to create dataset and data loader\n","def create_dataset_and_loader(data, word_to_idx, batch_size, shuffle):\n","    dataset = IMDBDataset(data, word_to_idx)\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n","    return dataset, loader"],"metadata":{"id":"VVaXfCgkW8hC","execution":{"iopub.status.busy":"2024-03-21T13:37:06.128496Z","iopub.execute_input":"2024-03-21T13:37:06.128852Z","iopub.status.idle":"2024-03-21T13:37:06.136888Z","shell.execute_reply.started":"2024-03-21T13:37:06.128820Z","shell.execute_reply":"2024-03-21T13:37:06.136018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","train_dataset, train_loader = create_dataset_and_loader(train_data, word_to_idx_train, batch_size, shuffle=True)\n","val_dataset, val_loader = create_dataset_and_loader(val_data, word_to_idx_train, batch_size, shuffle=False)\n","test_dataset, test_loader = create_dataset_and_loader(test_data, word_to_idx_train, batch_size, shuffle=False)\n","\n","# Print the lengths of datasets\n","print(\"Length of train_dataset:\", len(train_dataset))\n","print(\"Length of val_dataset:\", len(val_dataset))\n","print(\"Length of test_dataset:\", len(test_dataset))"],"metadata":{"id":"1yQt0oZmXDWZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710993106896,"user_tz":-330,"elapsed":8,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"4b147835-3a6d-486a-bf2f-887c48d7f84a","execution":{"iopub.status.busy":"2024-03-21T13:37:06.138122Z","iopub.execute_input":"2024-03-21T13:37:06.138406Z","iopub.status.idle":"2024-03-21T13:37:06.149996Z","shell.execute_reply.started":"2024-03-21T13:37:06.138383Z","shell.execute_reply":"2024-03-21T13:37:06.148932Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Length of train_dataset: 20056\nLength of val_dataset: 4004\nLength of test_dataset: 16020\n","output_type":"stream"}]},{"cell_type":"markdown","source":["#### 1.3 Create the Model class for the RNN Model. Create functions for running model training and testing."],"metadata":{"id":"SziN-P689_pZ"}},{"cell_type":"code","source":["# # create a model\n","# class RNNModel(nn.Module):\n","#   def __init__(self, vocab_size, hidden_size, embedding_dim, num_classes):\n","#     # call the init method of the parent\n","\n","#     # define the layers\n","\n","\n","#   def forward(self, X):\n","\n","#     # run foward pass through the model\n","\n","#     return logits"],"metadata":{"id":"GAldcBovUotd","execution":{"iopub.status.busy":"2024-03-21T13:37:06.151169Z","iopub.execute_input":"2024-03-21T13:37:06.151477Z","iopub.status.idle":"2024-03-21T13:37:06.161692Z","shell.execute_reply.started":"2024-03-21T13:37:06.151437Z","shell.execute_reply":"2024-03-21T13:37:06.160713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Hyperparameters\n","# hidden_size = 256\n","# embedding_dim = 128\n","# learning_rate = 1e-3\n","# epochs = 5\n","\n","# # create the model\n","# model = RNNModel(vocab_size, hidden_size, embedding_dim, num_classes).to(device)\n","\n","# # create optimizer\n","\n","# print(model)"],"metadata":{"id":"rndJltB9ZDlu","execution":{"iopub.status.busy":"2024-03-21T13:37:06.163054Z","iopub.execute_input":"2024-03-21T13:37:06.163489Z","iopub.status.idle":"2024-03-21T13:37:06.177370Z","shell.execute_reply.started":"2024-03-21T13:37:06.163453Z","shell.execute_reply":"2024-03-21T13:37:06.176295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Create a model training loop\n","# def train_model():\n","\n","#   for epoch in range(epochs):\n","#     ## TRAINING STEP\n","#     model.train()\n","#     # train\n","#     for input_batch, output_batch in tqdm(trainloader, total = len(trainloader), desc = \"Training\"):\n","\n","#     # Log metrics\n","\n","#     ## VALIDATION STEP\n","#     model.eval()\n","#     # run validation\n","#     for input_batch, output_batch in tqdm(valloader, total = len(valloader), desc = \"Validation\"):\n","\n","#     # Log metrics\n","\n","#     # store best model\n","\n","#   return train_losses, val_losses, val_accuracy"],"metadata":{"id":"SDVn7UEtZSKB","execution":{"iopub.status.busy":"2024-03-21T13:37:06.179238Z","iopub.execute_input":"2024-03-21T13:37:06.180168Z","iopub.status.idle":"2024-03-21T13:37:06.188255Z","shell.execute_reply.started":"2024-03-21T13:37:06.180125Z","shell.execute_reply":"2024-03-21T13:37:06.187060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a model testing loop\n"],"metadata":{"id":"cSiAyfTir5zy","execution":{"iopub.status.busy":"2024-03-21T13:37:06.189577Z","iopub.execute_input":"2024-03-21T13:37:06.189927Z","iopub.status.idle":"2024-03-21T13:37:06.198700Z","shell.execute_reply.started":"2024-03-21T13:37:06.189897Z","shell.execute_reply":"2024-03-21T13:37:06.197576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # train the model\n","# train_losses, val_losses, val_accuracy = train_model()"],"metadata":{"id":"3WA5MGp3aCFe","execution":{"iopub.status.busy":"2024-03-21T13:37:06.206080Z","iopub.execute_input":"2024-03-21T13:37:06.206542Z","iopub.status.idle":"2024-03-21T13:37:06.211124Z","shell.execute_reply.started":"2024-03-21T13:37:06.206504Z","shell.execute_reply":"2024-03-21T13:37:06.210051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot training and validation losses"],"metadata":{"id":"s7MRQCJTooPK","execution":{"iopub.status.busy":"2024-03-21T13:37:06.212255Z","iopub.execute_input":"2024-03-21T13:37:06.212554Z","iopub.status.idle":"2024-03-21T13:37:06.220602Z","shell.execute_reply.started":"2024-03-21T13:37:06.212530Z","shell.execute_reply":"2024-03-21T13:37:06.219589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot validation accuracy"],"metadata":{"id":"VLLkeoY59A5F","execution":{"iopub.status.busy":"2024-03-21T13:37:06.221888Z","iopub.execute_input":"2024-03-21T13:37:06.222273Z","iopub.status.idle":"2024-03-21T13:37:06.230443Z","shell.execute_reply.started":"2024-03-21T13:37:06.222229Z","shell.execute_reply":"2024-03-21T13:37:06.229538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the classification accuracy on test set\n"],"metadata":{"id":"gUjgUQhtoyQ0","execution":{"iopub.status.busy":"2024-03-21T13:37:06.231812Z","iopub.execute_input":"2024-03-21T13:37:06.232115Z","iopub.status.idle":"2024-03-21T13:37:06.240524Z","shell.execute_reply.started":"2024-03-21T13:37:06.232089Z","shell.execute_reply":"2024-03-21T13:37:06.239534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size):\n","        super(RNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)  # Output size is 1 for binary classification\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, _ = self.rnn(embedded)\n","        # Take the output from the last time step of the last layer\n","        last_output = output[:, -1, :]\n","        output = self.fc(last_output)\n","        return output.squeeze(1)  # Ensure output shape is [batch_size]\n"],"metadata":{"id":"qzU8P3d_FqGm","execution":{"iopub.status.busy":"2024-03-21T13:37:06.241659Z","iopub.execute_input":"2024-03-21T13:37:06.241958Z","iopub.status.idle":"2024-03-21T13:37:06.252578Z","shell.execute_reply.started":"2024-03-21T13:37:06.241935Z","shell.execute_reply":"2024-03-21T13:37:06.251639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training function\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.float().unsqueeze(1).to(device)  # Convert labels to float and unsqueeze to add a dimension\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels.view_as(outputs))  # View labels as the same shape as outputs\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * inputs.size(0)\n","\n","        train_loss /= len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.float().unsqueeze(1).to(device)  # Convert labels to float and unsqueeze to add a dimension\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels.view_as(outputs))  # View labels as the same shape as outputs\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Apply sigmoid activation for BCE\n","                predicted = torch.round(torch.sigmoid(outputs))\n","\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss /= len(val_loader.dataset)\n","        val_accuracy = correct / total\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","# Testing function\n","def test_model(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.float().unsqueeze(1).to(device)  # Convert labels to float and unsqueeze to add a dimension\n","            outputs = model(inputs)\n","            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to get predictions\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_accuracy = correct / total\n","    print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"id":"CGBpUG4uFqJA","execution":{"iopub.status.busy":"2024-03-21T13:37:06.254207Z","iopub.execute_input":"2024-03-21T13:37:06.254736Z","iopub.status.idle":"2024-03-21T13:37:06.273436Z","shell.execute_reply.started":"2024-03-21T13:37:06.254699Z","shell.execute_reply":"2024-03-21T13:37:06.272232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","input_size = len(word_to_idx_train)\n","embedding_size = 100\n","hidden_size = 128\n","num_layers = 1\n","output_size = 1  # For binary classification\n","learning_rate = 0.005\n","num_epochs = 5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize model, criterion, and optimizer\n","model2 = RNN(input_size, embedding_size, hidden_size, num_layers, output_size).to(device)\n","criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with logits\n","optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"],"metadata":{"id":"S9Z51NIYFqK_","execution":{"iopub.status.busy":"2024-03-21T13:37:06.274715Z","iopub.execute_input":"2024-03-21T13:37:06.275124Z","iopub.status.idle":"2024-03-21T13:37:08.706312Z","shell.execute_reply.started":"2024-03-21T13:37:06.275092Z","shell.execute_reply":"2024-03-21T13:37:08.705231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model2\n","train_model(model2, train_loader, val_loader, criterion, optimizer, num_epochs, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ASVm7wQFqMa","outputId":"3bb00194-78b3-4deb-f268-2562b5a423a2","execution":{"iopub.status.busy":"2024-03-21T13:37:08.707516Z","iopub.execute_input":"2024-03-21T13:37:08.708048Z","iopub.status.idle":"2024-03-21T13:43:26.383433Z","shell.execute_reply.started":"2024-03-21T13:37:08.708019Z","shell.execute_reply":"2024-03-21T13:43:26.382219Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.7029, Val Loss: 0.6929, Val Acc: 32.6783\nEpoch 2/5, Train Loss: 0.6980, Val Loss: 0.6941, Val Acc: 32.6783\nEpoch 3/5, Train Loss: 0.7010, Val Loss: 0.6954, Val Acc: 32.6783\nEpoch 4/5, Train Loss: 0.6979, Val Loss: 0.7144, Val Acc: 32.6783\nEpoch 5/5, Train Loss: 0.6997, Val Loss: 0.6933, Val Acc: 31.0699\n","output_type":"stream"}]},{"cell_type":"code","source":["# Test the model\n","test_model(model2, test_loader, device)"],"metadata":{"id":"z8WXoKnhGAII","execution":{"iopub.status.busy":"2024-03-21T13:43:26.384645Z","iopub.execute_input":"2024-03-21T13:43:26.384973Z","iopub.status.idle":"2024-03-21T13:44:15.362759Z","shell.execute_reply.started":"2024-03-21T13:43:26.384947Z","shell.execute_reply":"2024-03-21T13:44:15.361631Z"},"trusted":true,"outputId":"ebdbc029-cc4f-4dac-9e3f-86df0983c64e"},"execution_count":null,"outputs":[{"name":"stdout","text":"Test Accuracy: 31.4519\n","output_type":"stream"}]},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size):\n","        super(RNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)  # Output size is 1 for binary classification\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, _ = self.rnn(embedded)\n","        output_mean = torch.mean(output, dim=1)\n","        output = self.fc(output_mean)\n","        return output.squeeze(1)  # Ensure output shape is [batch_size]"],"metadata":{"id":"SpcAIW_dTI6b","execution":{"iopub.status.busy":"2024-03-21T13:44:15.364217Z","iopub.execute_input":"2024-03-21T13:44:15.364533Z","iopub.status.idle":"2024-03-21T13:44:15.373165Z","shell.execute_reply.started":"2024-03-21T13:44:15.364507Z","shell.execute_reply":"2024-03-21T13:44:15.372073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training function\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Convert labels to float\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * inputs.size(0)\n","\n","        train_loss /= len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.float().to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Apply sigmoid activation and round to get predictions\n","                predicted = torch.round(torch.sigmoid(outputs))\n","\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss /= len(val_loader.dataset)\n","        val_accuracy = correct / total\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","\n","# Testing function\n","def test_model(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Convert labels to float\n","            outputs = model(inputs)\n","            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to get predictions\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_accuracy = correct / total\n","    print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"id":"iCXniknYTNYj","execution":{"iopub.status.busy":"2024-03-21T13:44:15.374524Z","iopub.execute_input":"2024-03-21T13:44:15.374948Z","iopub.status.idle":"2024-03-21T13:44:15.391871Z","shell.execute_reply.started":"2024-03-21T13:44:15.374917Z","shell.execute_reply":"2024-03-21T13:44:15.390802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","input_size = len(word_to_idx_train)\n","embedding_size = 100\n","hidden_size = 128\n","num_layers = 1\n","output_size = 1  # For binary classification\n","learning_rate = 0.005\n","num_epochs = 5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize model, criterion, and optimizer\n","model = RNN(input_size, embedding_size, hidden_size, num_layers, output_size).to(device)\n","criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with logits\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"U50PnlJ3TNa7","execution":{"iopub.status.busy":"2024-03-21T13:44:15.393890Z","iopub.execute_input":"2024-03-21T13:44:15.394280Z","iopub.status.idle":"2024-03-21T13:44:15.486658Z","shell.execute_reply.started":"2024-03-21T13:44:15.394249Z","shell.execute_reply":"2024-03-21T13:44:15.485544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"],"metadata":{"id":"-HhA3U5fTUMD","execution":{"iopub.status.busy":"2024-03-21T13:44:15.488362Z","iopub.execute_input":"2024-03-21T13:44:15.489127Z","iopub.status.idle":"2024-03-21T13:50:31.288003Z","shell.execute_reply.started":"2024-03-21T13:44:15.489089Z","shell.execute_reply":"2024-03-21T13:50:31.286989Z"},"trusted":true,"outputId":"b61964f6-e2c0-4789-fef3-c7b4658ad1ed"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.7034, Val Loss: 0.6913, Val Acc: 0.5122\nEpoch 2/5, Train Loss: 0.6869, Val Loss: 0.6688, Val Acc: 0.5617\nEpoch 3/5, Train Loss: 0.4728, Val Loss: 0.4973, Val Acc: 0.8204\nEpoch 4/5, Train Loss: 0.3723, Val Loss: 0.4281, Val Acc: 0.8014\nEpoch 5/5, Train Loss: 0.2626, Val Loss: 0.5176, Val Acc: 0.8319\n","output_type":"stream"}]},{"cell_type":"code","source":["# Test the model\n","test_model(model, test_loader, device)"],"metadata":{"id":"e24AwlGTTU1i","execution":{"iopub.status.busy":"2024-03-21T13:50:31.289496Z","iopub.execute_input":"2024-03-21T13:50:31.289911Z","iopub.status.idle":"2024-03-21T13:51:19.863154Z","shell.execute_reply.started":"2024-03-21T13:50:31.289876Z","shell.execute_reply":"2024-03-21T13:51:19.862160Z"},"trusted":true,"outputId":"ed457cb9-74c4-4d80-9cb0-498ad3d92a5b"},"execution_count":null,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.8297\n","output_type":"stream"}]},{"cell_type":"markdown","source":["#### 1.4 Incorporate stemming/lemmatization when doing text preprocessing using the NLTK library. What changes do you observe in accuracy ?"],"metadata":{"id":"tHmA54UjvRh6"}},{"cell_type":"code","source":[],"metadata":{"id":"MMW-AMrYvRxO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1.5 In the Model class, experiment with only picking the last output and mean of all outputs in the RNN layer. What changes do you observe ?"],"metadata":{"id":"23hqq3w8o2q3"}},{"cell_type":"code","source":[],"metadata":{"id":"-oUlRgmZAm-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Hyperparameter Tuning\n","#### 2.1 Starting with the best configurations based on the above experiments, experiment with 5 different hyperparameter configurations. You can change the size of embedding layer, hidden state, batch in the dataloader.\n"],"metadata":{"id":"dw2dyrnkrnxa"}},{"cell_type":"code","source":[],"metadata":{"id":"M0EuYk3Krodp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2.2 Evaluate the performance of the configurations on the validation sets using metrics like accuracy and loss. Analyze the results."],"metadata":{"id":"bfZSMbBr8cGt"}},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size):\n","        super(RNN, self).__init__()\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, 1)  # Output size is 1 for binary classification\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, _ = self.rnn(embedded)\n","        output_mean = torch.mean(output, dim=1)\n","        output = self.fc(output_mean)\n","        return output.squeeze(1)  # Ensure output shape is [batch_size]"],"metadata":{"id":"O8luyMnj8cnl","execution":{"iopub.status.busy":"2024-03-21T13:51:19.864857Z","iopub.execute_input":"2024-03-21T13:51:19.865267Z","iopub.status.idle":"2024-03-21T13:51:19.873476Z","shell.execute_reply.started":"2024-03-21T13:51:19.865229Z","shell.execute_reply":"2024-03-21T13:51:19.872440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)  # Move model to GPU if available\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Move inputs and labels to GPU\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * inputs.size(0)\n","\n","        train_loss /= len(train_loader.dataset)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}')\n","\n","\n","def evaluate_model(model, val_loader, criterion):\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs = inputs.to(device)  # Move inputs to GPU\n","            labels = labels.float().to(device)  # Move labels to GPU\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item() * inputs.size(0)\n","            # _, predicted = torch.max(outputs, 1)\\\n","            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to get predictions\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    val_loss /= len(val_loader.dataset)\n","    val_accuracy = correct / total\n","\n","    return val_loss, val_accuracy\n"],"metadata":{"id":"o_PaMsekGdVy","execution":{"iopub.status.busy":"2024-03-21T13:51:19.875073Z","iopub.execute_input":"2024-03-21T13:51:19.875458Z","iopub.status.idle":"2024-03-21T13:51:19.889359Z","shell.execute_reply.started":"2024-03-21T13:51:19.875422Z","shell.execute_reply":"2024-03-21T13:51:19.888429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n","#     model.to(device)\n","#     for epoch in range(num_epochs):\n","#         model.train()\n","#         train_loss = 0.0\n","#         for inputs, labels in train_loader:\n","#             inputs, labels = inputs.to(device), labels.float().to(device)  # Convert labels to float\n","#             optimizer.zero_grad()\n","#             outputs = model(inputs)\n","#             loss = criterion(outputs, labels)\n","#             loss.backward()\n","#             optimizer.step()\n","#             train_loss += loss.item() * inputs.size(0)\n","\n","#         train_loss /= len(train_loader.dataset)\n","\n","#         # Validation\n","#         model.eval()\n","#         val_loss = 0.0\n","#         correct = 0\n","#         total = 0\n","#         with torch.no_grad():\n","#             for inputs, labels in val_loader:\n","#                 inputs, labels = inputs.to(device), labels.float().to(device)\n","#                 outputs = model(inputs)\n","#                 loss = criterion(outputs, labels)\n","#                 val_loss += loss.item() * inputs.size(0)\n","\n","#                 # Apply sigmoid activation and round to get predictions\n","#                 predicted = torch.round(torch.sigmoid(outputs))\n","\n","#                 total += labels.size(0)\n","#                 correct += (predicted == labels).sum().item()\n","\n","#         val_loss /= len(val_loader.dataset)\n","#         val_accuracy = correct / total\n","\n","#         print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')"],"metadata":{"id":"3yN3PoYCGVUh","execution":{"iopub.status.busy":"2024-03-21T13:51:19.890601Z","iopub.execute_input":"2024-03-21T13:51:19.891047Z","iopub.status.idle":"2024-03-21T13:51:19.907417Z","shell.execute_reply.started":"2024-03-21T13:51:19.891012Z","shell.execute_reply":"2024-03-21T13:51:19.906574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing function\n","def test_model(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Move data to GPU\n","            outputs = model(inputs)\n","            # _, predicted = torch.max(outputs, 1)\n","            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to get predictions\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_accuracy = correct / total\n","    print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"id":"Q8BxLjibVXzc","execution":{"iopub.status.busy":"2024-03-21T13:51:19.908669Z","iopub.execute_input":"2024-03-21T13:51:19.909098Z","iopub.status.idle":"2024-03-21T13:51:19.926773Z","shell.execute_reply.started":"2024-03-21T13:51:19.909059Z","shell.execute_reply":"2024-03-21T13:51:19.925529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","\n","# Define ranges for hyperparameters\n","input_size = len(word_to_idx_train)\n","embedding_sizes = [100, 200, 300]\n","hidden_sizes = [128, 256, 512]\n","batch_sizes = [32, 64, 128]\n","num_layers = [1] #[1, 2]\n","learning_rates = [0.005] # [0.001, 0.005]\n","\n","# Define other hyperparameters\n","output_size = 2\n","num_epochs = 5\n","\n","best_accuracy = 0.0\n","best_hyperparameters = {}\n","best_validation_loss = float('inf')\n","best_model_state = None\n","\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Iterate over all combinations of hyperparameters\n","for embedding_size, hidden_size, batch_size, num_layers, learning_rate in itertools.product(embedding_sizes, hidden_sizes, batch_sizes, num_layers, learning_rates):\n","    # Initialize model, criterion, and optimizer\n","    model = RNN(input_size, embedding_size, hidden_size, num_layers, output_size).to(device)  # Move model to GPU\n","#     criterion = nn.CrossEntropyLoss()\n","    criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with logits\n","\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    print(f\"Embedding Size: {embedding_size}, Hidden Size: {hidden_size}, Batch Size: {batch_size}, Num Layers: {num_layers}, Learning Rate: {learning_rate}\")\n","\n","    # Create train and validation data loaders with current batch size\n","    train_dataset, train_loader = create_dataset_and_loader(train_data, word_to_idx_train, batch_size, shuffle=True)\n","    val_dataset, val_loader = create_dataset_and_loader(val_data, word_to_idx_train, batch_size, shuffle=False)\n","\n","    # Train the model\n","    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n","\n","    # Evaluate the model on the validation set\n","    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n","\n","    # Print results\n","    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","    print()\n","    # Update best hyperparameters if current configuration performs better\n","    if val_accuracy > best_accuracy:\n","        best_accuracy = val_accuracy\n","        best_hyperparameters = {\n","            'embedding_size': embedding_size,\n","            'hidden_size': hidden_size,\n","            'batch_size': batch_size,\n","            'num_layers': num_layers,\n","            'learning_rate': learning_rate\n","        }\n","        # Save the state of the best model\n","        best_model_state = model.state_dict()\n","\n","    # Update best validation loss if current configuration has lower validation loss\n","    if val_loss < best_validation_loss:\n","        best_validation_loss = val_loss\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"outputId":"80927762-929e-4dbb-cbc9-f35efa74c222","id":"ZIff3-RHT2MZ","executionInfo":{"status":"error","timestamp":1710994620692,"user_tz":-330,"elapsed":1378356,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"scrolled":true,"execution":{"iopub.status.busy":"2024-03-21T13:51:19.928260Z","iopub.execute_input":"2024-03-21T13:51:19.928666Z","iopub.status.idle":"2024-03-21T16:31:54.775447Z","shell.execute_reply.started":"2024-03-21T13:51:19.928628Z","shell.execute_reply":"2024-03-21T16:31:54.774350Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Embedding Size: 100, Hidden Size: 128, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6914\nEpoch 2/5, Train Loss: 0.5360\nEpoch 3/5, Train Loss: 0.5117\nEpoch 4/5, Train Loss: 0.3285\nEpoch 5/5, Train Loss: 0.2614\nValidation Loss: 0.3742, Validation Accuracy: 0.8429\n\nEmbedding Size: 100, Hidden Size: 128, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6955\nEpoch 2/5, Train Loss: 0.6582\nEpoch 3/5, Train Loss: 0.6062\nEpoch 4/5, Train Loss: 0.5244\nEpoch 5/5, Train Loss: 0.4382\nValidation Loss: 0.5000, Validation Accuracy: 0.7892\n\nEmbedding Size: 100, Hidden Size: 128, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6725\nEpoch 2/5, Train Loss: 0.4889\nEpoch 3/5, Train Loss: 0.3225\nEpoch 4/5, Train Loss: 0.2309\nEpoch 5/5, Train Loss: 0.1988\nValidation Loss: 0.4675, Validation Accuracy: 0.8189\n\nEmbedding Size: 100, Hidden Size: 256, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6897\nEpoch 2/5, Train Loss: 0.6488\nEpoch 3/5, Train Loss: 0.5975\nEpoch 4/5, Train Loss: 0.2895\nEpoch 5/5, Train Loss: 0.2321\nValidation Loss: 0.3904, Validation Accuracy: 0.8352\n\nEmbedding Size: 100, Hidden Size: 256, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6994\nEpoch 2/5, Train Loss: 0.6490\nEpoch 3/5, Train Loss: 0.6220\nEpoch 4/5, Train Loss: 0.5253\nEpoch 5/5, Train Loss: 0.4668\nValidation Loss: 0.4954, Validation Accuracy: 0.7448\n\nEmbedding Size: 100, Hidden Size: 256, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6997\nEpoch 2/5, Train Loss: 0.6522\nEpoch 3/5, Train Loss: 0.6449\nEpoch 4/5, Train Loss: 0.5770\nEpoch 5/5, Train Loss: 0.4414\nValidation Loss: 0.4818, Validation Accuracy: 0.8267\n\nEmbedding Size: 100, Hidden Size: 512, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6958\nEpoch 2/5, Train Loss: 0.6896\nEpoch 3/5, Train Loss: 0.6888\nEpoch 4/5, Train Loss: 0.4504\nEpoch 5/5, Train Loss: 0.2633\nValidation Loss: 0.4281, Validation Accuracy: 0.8124\n\nEmbedding Size: 100, Hidden Size: 512, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6983\nEpoch 2/5, Train Loss: 0.6901\nEpoch 3/5, Train Loss: 0.6865\nEpoch 4/5, Train Loss: 0.7149\nEpoch 5/5, Train Loss: 0.6721\nValidation Loss: 0.6748, Validation Accuracy: 0.5977\n\nEmbedding Size: 100, Hidden Size: 512, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.7019\nEpoch 2/5, Train Loss: 0.6896\nEpoch 3/5, Train Loss: 0.6724\nEpoch 4/5, Train Loss: 0.6837\nEpoch 5/5, Train Loss: 0.6797\nValidation Loss: 0.6806, Validation Accuracy: 0.5529\n\nEmbedding Size: 200, Hidden Size: 128, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.5788\nEpoch 2/5, Train Loss: 0.3391\nEpoch 3/5, Train Loss: 0.1931\nEpoch 4/5, Train Loss: 0.1290\nEpoch 5/5, Train Loss: 0.1120\nValidation Loss: 0.5403, Validation Accuracy: 0.8137\n\nEmbedding Size: 200, Hidden Size: 128, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6847\nEpoch 2/5, Train Loss: 0.4667\nEpoch 3/5, Train Loss: 0.3076\nEpoch 4/5, Train Loss: 0.1904\nEpoch 5/5, Train Loss: 0.1489\nValidation Loss: 0.4612, Validation Accuracy: 0.8204\n\nEmbedding Size: 200, Hidden Size: 128, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6963\nEpoch 2/5, Train Loss: 0.6660\nEpoch 3/5, Train Loss: 0.4826\nEpoch 4/5, Train Loss: 0.2960\nEpoch 5/5, Train Loss: 0.1838\nValidation Loss: 0.4211, Validation Accuracy: 0.8252\n\nEmbedding Size: 200, Hidden Size: 256, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6534\nEpoch 2/5, Train Loss: 0.4464\nEpoch 3/5, Train Loss: 0.3972\nEpoch 4/5, Train Loss: 0.2438\nEpoch 5/5, Train Loss: 0.2299\nValidation Loss: 0.4607, Validation Accuracy: 0.8142\n\nEmbedding Size: 200, Hidden Size: 256, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6992\nEpoch 2/5, Train Loss: 0.6967\nEpoch 3/5, Train Loss: 0.6819\nEpoch 4/5, Train Loss: 0.5482\nEpoch 5/5, Train Loss: 0.5044\nValidation Loss: 0.5360, Validation Accuracy: 0.7113\n\nEmbedding Size: 200, Hidden Size: 256, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6865\nEpoch 2/5, Train Loss: 0.5190\nEpoch 3/5, Train Loss: 0.3703\nEpoch 4/5, Train Loss: 0.2466\nEpoch 5/5, Train Loss: 0.1731\nValidation Loss: 0.5549, Validation Accuracy: 0.8014\n\nEmbedding Size: 200, Hidden Size: 512, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.7143\nEpoch 2/5, Train Loss: 0.6347\nEpoch 3/5, Train Loss: 0.4675\nEpoch 4/5, Train Loss: 0.3689\nEpoch 5/5, Train Loss: 0.2993\nValidation Loss: 0.5258, Validation Accuracy: 0.7690\n\nEmbedding Size: 200, Hidden Size: 512, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.7032\nEpoch 2/5, Train Loss: 0.6963\nEpoch 3/5, Train Loss: 0.6814\nEpoch 4/5, Train Loss: 0.6486\nEpoch 5/5, Train Loss: 0.6130\nValidation Loss: 0.6440, Validation Accuracy: 0.5769\n\nEmbedding Size: 200, Hidden Size: 512, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.7091\nEpoch 2/5, Train Loss: 0.6795\nEpoch 3/5, Train Loss: 0.6668\nEpoch 4/5, Train Loss: 0.6890\nEpoch 5/5, Train Loss: 0.6901\nValidation Loss: 0.6912, Validation Accuracy: 0.5210\n\nEmbedding Size: 300, Hidden Size: 128, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.5570\nEpoch 2/5, Train Loss: 0.3195\nEpoch 3/5, Train Loss: 0.1776\nEpoch 4/5, Train Loss: 0.1126\nEpoch 5/5, Train Loss: 0.0787\nValidation Loss: 0.5205, Validation Accuracy: 0.8162\n\nEmbedding Size: 300, Hidden Size: 128, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6734\nEpoch 2/5, Train Loss: 0.3765\nEpoch 3/5, Train Loss: 0.2233\nEpoch 4/5, Train Loss: 0.1464\nEpoch 5/5, Train Loss: 0.0915\nValidation Loss: 0.4881, Validation Accuracy: 0.8164\n\nEmbedding Size: 300, Hidden Size: 128, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6743\nEpoch 2/5, Train Loss: 0.6018\nEpoch 3/5, Train Loss: 0.4457\nEpoch 4/5, Train Loss: 0.3727\nEpoch 5/5, Train Loss: 0.2730\nValidation Loss: 0.5095, Validation Accuracy: 0.8299\n\nEmbedding Size: 300, Hidden Size: 256, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6330\nEpoch 2/5, Train Loss: 0.4517\nEpoch 3/5, Train Loss: 0.2778\nEpoch 4/5, Train Loss: 0.1674\nEpoch 5/5, Train Loss: 0.1185\nValidation Loss: 0.4151, Validation Accuracy: 0.8294\n\nEmbedding Size: 300, Hidden Size: 256, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6995\nEpoch 2/5, Train Loss: 0.4465\nEpoch 3/5, Train Loss: 0.3436\nEpoch 4/5, Train Loss: 0.2040\nEpoch 5/5, Train Loss: 0.1546\nValidation Loss: 0.5043, Validation Accuracy: 0.8234\n\nEmbedding Size: 300, Hidden Size: 256, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6768\nEpoch 2/5, Train Loss: 0.5110\nEpoch 3/5, Train Loss: 0.4741\nEpoch 4/5, Train Loss: 0.2588\nEpoch 5/5, Train Loss: 0.1996\nValidation Loss: 0.4466, Validation Accuracy: 0.8397\n\nEmbedding Size: 300, Hidden Size: 512, Batch Size: 32, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6661\nEpoch 2/5, Train Loss: 0.4607\nEpoch 3/5, Train Loss: 0.4008\nEpoch 4/5, Train Loss: 0.3008\nEpoch 5/5, Train Loss: 0.3394\nValidation Loss: 0.4348, Validation Accuracy: 0.8272\n\nEmbedding Size: 300, Hidden Size: 512, Batch Size: 64, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.7103\nEpoch 2/5, Train Loss: 0.7001\nEpoch 3/5, Train Loss: 0.6948\nEpoch 4/5, Train Loss: 0.6933\nEpoch 5/5, Train Loss: 0.6027\nValidation Loss: 0.5907, Validation Accuracy: 0.6823\n\nEmbedding Size: 300, Hidden Size: 512, Batch Size: 128, Num Layers: 1, Learning Rate: 0.005\nEpoch 1/5, Train Loss: 0.6991\nEpoch 2/5, Train Loss: 0.6217\nEpoch 3/5, Train Loss: 0.6301\nEpoch 4/5, Train Loss: 0.5481\nEpoch 5/5, Train Loss: 0.4450\nValidation Loss: 0.4732, Validation Accuracy: 0.8227\n\n","output_type":"stream"}]},{"cell_type":"code","source":["# import itertools\n","\n","# # Define ranges for hyperparameters\n","# embedding_sizes = [200, 300]\n","# hidden_sizes = [128, 256]\n","# batch_sizes = [32, 64]\n","# num_layers = [1, 2]\n","# learning_rates = [0.001, 0.005]\n","\n","# # Define other hyperparameters\n","# output_size = 2\n","# num_epochs = 5\n","\n","# best_accuracy = 0.0\n","# best_hyperparameters = {}\n","# best_validation_loss = float('inf')\n","# best_model_state = None\n","# best_model_path = 'best_model.pth'  # Path to save the best model\n","\n","# # Check if GPU is available\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # Iterate over all combinations of hyperparameters\n","# for embedding_size, hidden_size, batch_size, num_layers, learning_rate in itertools.product(embedding_sizes, hidden_sizes, batch_sizes, num_layers, learning_rates):\n","#     # Initialize model, criterion, and optimizer\n","#     model = RNN(input_size, embedding_size, hidden_size, num_layers, output_size).to(device)  # Move model to GPU\n","#     criterion = nn.CrossEntropyLoss()\n","#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","#     # Create train and validation data loaders with current batch size\n","#     train_dataset, train_loader = create_dataset_and_loader(train_data, word_to_idx_train, batch_size, shuffle=True)\n","#     val_dataset, val_loader = create_dataset_and_loader(val_data, word_to_idx_train, batch_size, shuffle=False)\n","\n","#     # Train the model\n","#     train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n","\n","#     # Evaluate the model on the validation set\n","#     val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n","\n","#     # Print results\n","#     print(f\"Embedding Size: {embedding_size}, Hidden Size: {hidden_size}, Batch Size: {batch_size}, Num Layers: {num_layers}, Learning Rate: {learning_rate}\")\n","#     print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","\n","#     # Update best hyperparameters if current configuration performs better\n","#     if val_accuracy > best_accuracy:\n","#         best_accuracy = val_accuracy\n","#         best_hyperparameters = {\n","#             'embedding_size': embedding_size,\n","#             'hidden_size': hidden_size,\n","#             'batch_size': batch_size,\n","#             'num_layers': num_layers,\n","#             'learning_rate': learning_rate\n","#         }\n","#         # Save the state of the best model\n","#         best_model_state = model.state_dict()\n","\n","#     # Update best validation loss if current configuration has lower validation loss\n","#     if val_loss < best_validation_loss:\n","#         best_validation_loss = val_loss\n","\n","# # Save the best model's state dictionary to a file\n","# if best_model_state:\n","#     torch.save(best_model_state, best_model_path)\n","\n","# print(\"Best Hyperparameters:\", best_hyperparameters)\n","# print(\"Best Validation Loss:\", best_validation_loss)\n","\n","# # Load the best model for testing\n","# best_model = RNN(input_size, best_hyperparameters['embedding_size'], best_hyperparameters['hidden_size'], best_hyperparameters['num_layers'], output_size).to(device)\n","# best_model.load_state_dict(torch.load(best_model_path))"],"metadata":{"id":"c8e0KowdGAhJ","_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-03-21T16:31:54.777244Z","iopub.execute_input":"2024-03-21T16:31:54.777626Z","iopub.status.idle":"2024-03-21T16:31:54.785315Z","shell.execute_reply.started":"2024-03-21T16:31:54.777593Z","shell.execute_reply":"2024-03-21T16:31:54.784276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"rEqMRXzru0fF"}},{"cell_type":"code","source":["best_model_path = 'best_model.pt'  # Path to save the best model\n","# Save the best model's state dictionary to a file\n","if best_model_state:\n","    torch.save(best_model_state, best_model_path)\n","print()\n","print(\"Best Hyperparameters:\", best_hyperparameters)\n","print(\"Best Validation Loss:\", best_validation_loss)\n","print()\n","# Load the best model for testing\n","best_model = RNN(input_size, best_hyperparameters['embedding_size'], best_hyperparameters['hidden_size'], best_hyperparameters['num_layers'], output_size).to(device)\n","best_model.load_state_dict(torch.load(best_model_path))\n","\n","# Test the model\n","test_model(best_model, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"761255f0-2b3c-4cc5-8a8a-2c1437f09a97","id":"YFweoIN6ImYR","execution":{"iopub.status.busy":"2024-03-21T16:48:33.300455Z","iopub.execute_input":"2024-03-21T16:48:33.300883Z","iopub.status.idle":"2024-03-21T16:49:20.320854Z","shell.execute_reply.started":"2024-03-21T16:48:33.300851Z","shell.execute_reply":"2024-03-21T16:49:20.319774Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\nBest Hyperparameters: {'embedding_size': 100, 'hidden_size': 128, 'batch_size': 32, 'num_layers': 1, 'learning_rate': 0.005}\nBest Validation Loss: 0.3742440557860947\n\nTest Accuracy: 0.8361\n","output_type":"stream"}]},{"cell_type":"markdown","source":["**Best Hyperparameters:** {'embedding_size': 100, 'hidden_size': 128, 'batch_size': 32, 'num_layers': 1, 'learning_rate': 0.005}\n","\n","**Best Validation Loss:** 0.3742440557860947\n","\n","**Validation Accuracy:** 0.8361  "],"metadata":{"id":"GEU-dTp_WB6W"}},{"cell_type":"markdown","source":["### 3. After RNNs\n","#### 3.1 Keeping all the parameters same, replace the RNN layer with the LSTM layer using nn.LSTM. What changes do you observe ? Explain why LSTM layer would affect performance."],"metadata":{"id":"791zabclpSgi"}},{"cell_type":"code","source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size):\n","        super(LSTMModel, self).__init__()  # Corrected super() call\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)  # Replaced RNN with LSTM\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, _ = self.rnn(embedded)\n","        # Take the output from the last time step of the last layer\n","        last_output = output[:, -1, :]\n","        output = self.fc(last_output)\n","        return output.squeeze(1)  # Squeeze to make output shape (batch_size,)"],"metadata":{"id":"lkbOdAMUrpgl","execution":{"iopub.status.busy":"2024-03-21T16:32:44.784298Z","iopub.execute_input":"2024-03-21T16:32:44.784781Z","iopub.status.idle":"2024-03-21T16:32:44.792556Z","shell.execute_reply.started":"2024-03-21T16:32:44.784727Z","shell.execute_reply":"2024-03-21T16:32:44.791515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training function\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Convert labels to float\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * inputs.size(0)\n","\n","        train_loss /= len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.float().to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Apply sigmoid activation and round to get predictions\n","                predicted = torch.round(torch.sigmoid(outputs))\n","\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss /= len(val_loader.dataset)\n","        val_accuracy = correct / total\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","\n","# Testing function\n","def test_model(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Convert labels to float\n","            outputs = model(inputs)\n","            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to get predictions\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_accuracy = correct / total\n","    print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"id":"w8OybEKKY9EM","execution":{"iopub.status.busy":"2024-03-21T16:32:44.793995Z","iopub.execute_input":"2024-03-21T16:32:44.794321Z","iopub.status.idle":"2024-03-21T16:32:44.810177Z","shell.execute_reply.started":"2024-03-21T16:32:44.794275Z","shell.execute_reply":"2024-03-21T16:32:44.809204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","input_size = len(word_to_idx_train)\n","embedding_size = 100\n","hidden_size = 128\n","num_layers = 1  # Adjusted for simplicity\n","output_size = 1  # Output size changed to 1 for binary classification\n","learning_rate = 0.005\n","num_epochs = 5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize model, criterion, and optimizer\n","model2 = LSTMModel(input_size, embedding_size, hidden_size, num_layers, output_size).to(device)\n","criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy loss\n","optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"],"metadata":{"id":"axBj-QrZLLY_","execution":{"iopub.status.busy":"2024-03-21T16:32:44.811586Z","iopub.execute_input":"2024-03-21T16:32:44.812660Z","iopub.status.idle":"2024-03-21T16:32:44.899604Z","shell.execute_reply.started":"2024-03-21T16:32:44.812630Z","shell.execute_reply":"2024-03-21T16:32:44.898565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","train_model(model2, train_loader, val_loader, criterion, optimizer, num_epochs, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPf-xymYLN_n","outputId":"3022b83a-51cc-435f-9981-4203d1545070","execution":{"iopub.status.busy":"2024-03-21T16:32:44.900812Z","iopub.execute_input":"2024-03-21T16:32:44.901106Z","iopub.status.idle":"2024-03-21T16:39:09.505973Z","shell.execute_reply.started":"2024-03-21T16:32:44.901080Z","shell.execute_reply":"2024-03-21T16:39:09.504833Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6956, Val Loss: 0.6929, Val Acc: 0.5122\nEpoch 2/5, Train Loss: 0.6930, Val Loss: 0.6929, Val Acc: 0.5122\nEpoch 3/5, Train Loss: 0.6930, Val Loss: 0.6932, Val Acc: 0.4878\nEpoch 4/5, Train Loss: 0.6930, Val Loss: 0.6929, Val Acc: 0.5122\nEpoch 5/5, Train Loss: 0.6930, Val Loss: 0.6929, Val Acc: 0.5122\n","output_type":"stream"}]},{"cell_type":"code","source":["# Test the model\n","test_model(model2, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"64a691d2-7daf-43f9-8bea-1a32e99f32aa","id":"kksdv01dlpz8","executionInfo":{"status":"ok","timestamp":1710967177803,"user_tz":-330,"elapsed":377,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"execution":{"iopub.status.busy":"2024-03-21T16:39:09.507147Z","iopub.execute_input":"2024-03-21T16:39:09.507470Z","iopub.status.idle":"2024-03-21T16:39:58.049294Z","shell.execute_reply.started":"2024-03-21T16:39:09.507442Z","shell.execute_reply":"2024-03-21T16:39:58.048311Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.5082\n","output_type":"stream"}]},{"cell_type":"code","source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size):\n","        super(LSTMModel, self).__init__()  # Corrected super() call\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)  # Replaced RNN with LSTM\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        output, _ = self.rnn(embedded)\n","        output_mean = torch.mean(output, dim=1)\n","        output = self.fc(output_mean)\n","        return output.squeeze(1)  # Ensure output shape is [batch_size]"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T16:39:58.050459Z","iopub.execute_input":"2024-03-21T16:39:58.050769Z","iopub.status.idle":"2024-03-21T16:39:58.058283Z","shell.execute_reply.started":"2024-03-21T16:39:58.050731Z","shell.execute_reply":"2024-03-21T16:39:58.057280Z"},"trusted":true,"id":"cV-thoa4RHrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training function\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Convert labels to float\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * inputs.size(0)\n","\n","        train_loss /= len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.float().to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Apply sigmoid activation and round to get predictions\n","                predicted = torch.round(torch.sigmoid(outputs))\n","\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss /= len(val_loader.dataset)\n","        val_accuracy = correct / total\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","\n","# Testing function\n","def test_model(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.float().to(device)  # Convert labels to float\n","            outputs = model(inputs)\n","            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to get predictions\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_accuracy = correct / total\n","    print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T16:39:58.059629Z","iopub.execute_input":"2024-03-21T16:39:58.059970Z","iopub.status.idle":"2024-03-21T16:39:58.075670Z","shell.execute_reply.started":"2024-03-21T16:39:58.059936Z","shell.execute_reply":"2024-03-21T16:39:58.074945Z"},"trusted":true,"id":"3qok3mlLRHrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","input_size = len(word_to_idx_train)\n","embedding_size = 100\n","hidden_size = 128\n","num_layers = 1  # Adjusted for simplicity\n","output_size = 1  # Output size changed to 1 for binary classification\n","learning_rate = 0.005\n","num_epochs = 5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize model, criterion, and optimizer\n","model = LSTMModel(input_size, embedding_size, hidden_size, num_layers, output_size).to(device)\n","criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy loss\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T16:39:58.076929Z","iopub.execute_input":"2024-03-21T16:39:58.077260Z","iopub.status.idle":"2024-03-21T16:39:58.155941Z","shell.execute_reply.started":"2024-03-21T16:39:58.077226Z","shell.execute_reply":"2024-03-21T16:39:58.155129Z"},"trusted":true,"id":"BtuDSSspRHrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T16:39:58.157600Z","iopub.execute_input":"2024-03-21T16:39:58.157951Z","iopub.status.idle":"2024-03-21T16:46:21.527222Z","shell.execute_reply.started":"2024-03-21T16:39:58.157920Z","shell.execute_reply":"2024-03-21T16:46:21.526251Z"},"trusted":true,"id":"w13mmKzORHrl","outputId":"00628fe6-4b2d-49da-e85d-1c6ff24cde9a"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.5386, Val Loss: 0.4131, Val Acc: 0.8359\nEpoch 2/5, Train Loss: 0.2362, Val Loss: 0.3274, Val Acc: 0.8666\nEpoch 3/5, Train Loss: 0.1245, Val Loss: 0.3958, Val Acc: 0.8649\nEpoch 4/5, Train Loss: 0.0667, Val Loss: 0.4853, Val Acc: 0.8511\nEpoch 5/5, Train Loss: 0.0322, Val Loss: 0.5438, Val Acc: 0.8467\n","output_type":"stream"}]},{"cell_type":"code","source":["# Test the model\n","test_model(model, test_loader, device)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T16:46:21.528497Z","iopub.execute_input":"2024-03-21T16:46:21.528804Z","iopub.status.idle":"2024-03-21T16:47:10.052405Z","shell.execute_reply.started":"2024-03-21T16:46:21.528763Z","shell.execute_reply":"2024-03-21T16:47:10.051418Z"},"trusted":true,"id":"W02k7VoJRHrl","outputId":"49e2db2a-5c97-4535-aca0-8dc5b589fe82"},"execution_count":null,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.8391\n","output_type":"stream"}]},{"cell_type":"code","source":["!ls -plia"],"metadata":{"execution":{"iopub.status.busy":"2024-03-21T16:49:49.034901Z","iopub.execute_input":"2024-03-21T16:49:49.035271Z","iopub.status.idle":"2024-03-21T16:49:50.058901Z","shell.execute_reply.started":"2024-03-21T16:49:49.035245Z","shell.execute_reply":"2024-03-21T16:49:50.057837Z"},"trusted":true,"id":"Q16KwN3TRHrl","outputId":"f81b5e4e-0b7f-4a72-b8a1-051759a1d08c"},"execution_count":null,"outputs":[{"name":"stdout","text":"total 45772\n   786438 drwxr-xr-x 5 root root     4096 Mar 21 16:48 ./\n271237110 drwxr-xr-x 5 root root     4096 Mar 21 13:34 ../\n   786439 drwxr-xr-x 2 root root     4096 Mar 21 13:34 .virtual_documents/\n   786520 -rw-r--r-- 1 root root 23421864 Mar 21 16:48 best_model.pt\n   786519 -rw-r--r-- 1 root root 23421864 Mar 21 16:31 best_model.pth\n   786485 drwxr-xr-x 3 root root     4096 Mar 21 13:36 corpora/\n   786440 drwxr-xr-x 3 root root     4096 Mar 21 13:36 tokenizers/\n","output_type":"stream"}]},{"cell_type":"markdown","source":["**Incorporate stemming/lemmatization when doing text preprocessing using the NLTK library. What changes do you observe?**\n","\n","Incorporating stemming or lemmatization using NLTK can help by:  \n","\n","1. Normalization: Reducing words to their base forms, reducing vocabulary size.\n","2. Generalization: Treating different word forms as equivalent, improving model generalization.\n","3. Reduced Sparsity: Collapsing similar words, making training more efficient.\n","4. Loss of Information: Stemming or lemmatizing may lose some distinctions between words.\n","5. Computational Efficiency: Fewer unique words mean faster processing during training and inference.\n","\n","Overall, stemming/lemmatization can improve text representations, reduce complexity, and enhance model performance, but may lose some word distinctions."],"metadata":{"id":"FuXMwI5R6sgY"}},{"cell_type":"markdown","source":["**Observation:** In the Model class, when only picking the last output of the RNN layer, the model might not capture the sequential information effectively, especially in longer sequences. This approach essentially treats each sequence as a single data point, ignoring the sequential nature of the data.  \n","\n","On the other hand, when taking the mean of all outputs in the RNN layer, the model can capture the overall representation of the entire sequence. By considering the average representation of the entire sequence, the model might better capture the sequential patterns and dependencies present in the data.  \n","\n","Therefore, by considering the mean of all outputs in the RNN layer, the model might achieve slightly better performance as it can leverage the sequential information more effectively compared to only picking the last output. This difference in performance is reflected in the slightly higher test accuracy obtained when taking the mean of all outputs compared to only picking the last output.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"tab_R30Y5dEq"}},{"cell_type":"markdown","source":["**Reason:** The main advantage of LSTM over traditional RNN is its ability to effectively handle the vanishing gradient problem, which occurs when training RNNs on long sequences. This problem arises because traditional RNNs have difficulty retaining information over many time steps due to the repeated multiplication of gradients during backpropagation. LSTMs address this issue by introducing a memory cell that can store information for long periods, allowing the network to learn dependencies over longer sequences without suffering from vanishing gradients."],"metadata":{"id":"CTtvI7y35mHM"}},{"cell_type":"markdown","source":["Based on the provided results, we can analyze the performance of different hyperparameter configurations:\n","\n","1. **Embedding Size**:\n","   - Increasing the embedding size from 100 to 200 or 300 generally improves performance across different batch sizes and hidden sizes.\n","   - Higher embedding sizes allow the model to capture more complex relationships between words, which can lead to better performance.\n","\n","2. **Hidden Size**:\n","   - Increasing the hidden size from 128 to 256 or 512 also generally improves performance.\n","   - Larger hidden sizes allow the model to learn more complex patterns in the data, potentially leading to better performance.\n","   - However, very large hidden sizes may lead to overfitting, as seen in some cases where validation accuracy decreases.\n","\n","3. **Batch Size**:\n","   - The effect of batch size is less consistent across different configurations.\n","   - In some cases, smaller batch sizes (e.g., 32) lead to better performance, while in others, larger batch sizes (e.g., 128) perform better.\n","   - Smaller batch sizes may allow the model to converge faster due to more frequent updates, but they can also lead to slower training overall.\n","   - Larger batch sizes may provide more stable gradients and faster training, but they may also lead to convergence to suboptimal solutions.\n","\n","4. **Learning Rate**:\n","   - The learning rate is not explicitly varied in the provided results, so it's assumed to be constant.\n","   - It's important to note that the learning rate interacts with other hyperparameters and can significantly impact training dynamics and final performance.\n","   - A learning rate that's too high can lead to unstable training or divergence, while a learning rate that's too low can lead to slow convergence or getting stuck in local minima.\n","\n","5. **Number of Layers**:\n","   - The number of layers is kept constant in the provided results (1 layer).\n","   - Experimenting with the number of layers could provide further insights into the model's capacity to learn complex representations, but it might also increase training time and risk overfitting.\n","\n","6. **Validation Loss and Accuracy**:\n","   - Validation loss and accuracy are used as metrics to evaluate model performance.\n","   - Lower validation loss and higher accuracy indicate better model performance on unseen data."],"metadata":{"id":"PktXjXNt7YBj"}}]}