{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"id":"K6W0TP9kKvHA","execution":{"iopub.status.busy":"2024-03-22T07:14:02.269828Z","iopub.execute_input":"2024-03-22T07:14:02.270170Z","iopub.status.idle":"2024-03-22T07:14:03.224562Z","shell.execute_reply.started":"2024-03-22T07:14:02.270142Z","shell.execute_reply":"2024-03-22T07:14:03.223800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# import torchvision\n","# import torchvision.transforms as transforms\n","\n","# # Define transformation\n","# transform = transforms.Compose([\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","\n","# # Load CIFAR-10 dataset\n","# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","# trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","# testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n","\n","# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# import torchvision\n","# import torchvision.transforms as transforms\n","\n","# # Define transformation\n","# transform = torchvision.transforms.Compose([\n","#     torchvision.transforms.ToTensor(),\n","#     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","\n","# # Load CIFAR-10 dataset\n","# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","# trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","# testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","# from torchvision import models, datasets, transforms\n","\n","# Set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define transforms for the dataset\n","transform = torchvision.transforms.Compose([\n","    # torchvision.transforms.Resize(224),\n","#     torchvision.transforms.Resize((224,224)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","# Load CIFAR-10 dataset\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n","\n"],"metadata":{"id":"GIlzXi9y_kKv","execution":{"iopub.status.busy":"2024-03-22T07:45:43.639366Z","iopub.execute_input":"2024-03-22T07:45:43.639968Z","iopub.status.idle":"2024-03-22T07:46:03.122917Z","shell.execute_reply.started":"2024-03-22T07:45:43.639936Z","shell.execute_reply":"2024-03-22T07:46:03.122117Z"},"trusted":true,"outputId":"ded2fd88-5d70-4f52-c006-1c3b790d32b0"},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:08<00:00, 19423476.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":["# Define MLP model\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Initialize model, loss function, and optimizer\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","mlp_model = MLP().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n","\n","# Training the model\n","for epoch in range(5):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = mlp_model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n","\n","# Evaluate the model\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        outputs = mlp_model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"],"metadata":{"id":"DNigPnw7ho-l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710796259578,"user_tz":-330,"elapsed":169789,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"f333cfc1-e635-43ba-96a7-ebf59017f82d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Files already downloaded and verified\n\nFiles already downloaded and verified\n\nEpoch 1, Loss: 1.6527410385247157\n\nEpoch 2, Loss: 1.4614308119506616\n\nEpoch 3, Loss: 1.357220009245784\n\nEpoch 4, Loss: 1.270260181056332\n\nEpoch 5, Loss: 1.1911689409901527\n\nEpoch 6, Loss: 1.1269549683584896\n\nEpoch 7, Loss: 1.0574596153949973\n\nEpoch 8, Loss: 0.9986916581598979\n\nEpoch 9, Loss: 0.940253328472395\n\nEpoch 10, Loss: 0.8878760070695529\n\nAccuracy of the network on the 10000 test images: 52 %\n"}]},{"cell_type":"code","source":["# Define CNN model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n","        self.fc2 = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = self.pool(torch.relu(self.conv3(x)))\n","        x = x.view(-1, 64 * 4 * 4)\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Initialize model, loss function, and optimizer\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cnn_model = CNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n","\n","# Training the model\n","for epoch in range(5):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = cnn_model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n","\n","# Evaluate the model\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        outputs = cnn_model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEAOdmg3Pmgl","executionInfo":{"status":"ok","timestamp":1710788671884,"user_tz":-330,"elapsed":193710,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"a2d28445-8ee4-47b3-88b5-50bd2063dd96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Files already downloaded and verified\n\nFiles already downloaded and verified\n\nEpoch 1, Loss: 1.3555726040195213\n\nEpoch 2, Loss: 0.9396836510165257\n\nEpoch 3, Loss: 0.7715283925725493\n\nEpoch 4, Loss: 0.6535785308993175\n\nEpoch 5, Loss: 0.5640925135146481\n\nEpoch 6, Loss: 0.48523526733606503\n\nEpoch 7, Loss: 0.41329072573611314\n\nEpoch 8, Loss: 0.348453672522928\n\nEpoch 9, Loss: 0.2954901914545457\n\nEpoch 10, Loss: 0.2534648234278478\n\nAccuracy of the network on the 10000 test images: 74 %\n"}]},{"cell_type":"code","source":["# Define transforms for the dataset\n","transform = torchvision.transforms.Compose([\n","    # torchvision.transforms.Resize(224),\n","    torchvision.transforms.Resize((224,224)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","# Load CIFAR-10 dataset\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n","\n","# Load pre-trained VGG19 model\n","vgg19 = torchvision.models.vgg19(weights='VGG19_Weights.DEFAULT')\n","print(vgg19.features)\n","print(vgg19.features.parameters())\n","print(vgg19.classifier)\n","\n","# Load pre-trained VGG16 model\n","vgg16 = torchvision.models.vgg16(weights='VGG16_Weights.DEFAULT')\n","print(vgg16.features)\n","print(vgg16.features.parameters())\n","print(vgg16.classifier)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:58:07.843435Z","iopub.execute_input":"2024-03-22T07:58:07.844005Z","iopub.status.idle":"2024-03-22T07:58:16.497658Z","shell.execute_reply.started":"2024-03-22T07:58:07.843964Z","shell.execute_reply":"2024-03-22T07:58:16.496728Z"},"trusted":true,"id":"crZSZOX7QsKD","outputId":"4e4783ac-4555-4c03-d9a1-a33d41a81b5e"},"execution_count":null,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (17): ReLU(inplace=True)\n  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (24): ReLU(inplace=True)\n  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (26): ReLU(inplace=True)\n  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (31): ReLU(inplace=True)\n  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (33): ReLU(inplace=True)\n  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (35): ReLU(inplace=True)\n  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n<generator object Module.parameters at 0x7cb1dacea570>\nSequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 161MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n<generator object Module.parameters at 0x7cb1dacea030>\nSequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":["# Freeze convolutional layers\n","for param in vgg19.features.parameters():\n","    param.requires_grad = False\n","\n","# Modify the last fully connected layer to output 10 classes\n","# num_features = vgg19.classifier[6].in_features\n","# features = list(vgg19.classifier.children())[:-1]\n","# features.extend([nn.Linear(num_features, 10)])\n","# vgg19.classifier = nn.Sequential(*features)\n","vgg19.classifier[6] = nn.Linear(vgg19.classifier[6].in_features, 10)\n","\n","\n","# Transfer model to device\n","vgg19.to(device)\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(vgg19.parameters(), lr=0.001)\n","\n","# Train the model\n","epochs = 5\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    # for inputs, labels in trainloader:\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = vgg19(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        # if i % 100 == 99:    # print every 100 mini-batches\n","    print('epoch %d loss: %f' % (epoch + 1, running_loss / len(trainloader)))\n","        # running_loss = 0.0\n","\n","# Evaluate the model\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        outputs = vgg19(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2RkAowFVPoaF","executionInfo":{"status":"ok","timestamp":1710796037167,"user_tz":-330,"elapsed":3003130,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"7aa0dc79-2d9e-4b20-917d-2c0f0b022609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Files already downloaded and verified\n\nFiles already downloaded and verified\n\n1 epoch loss: 0.942\n\n2 epoch loss: 0.726\n\n3 epoch loss: 0.640\n\n4 epoch loss: 0.576\n\n5 epoch loss: 0.524\n\n6 epoch loss: 0.467\n\n7 epoch loss: 0.450\n\n8 epoch loss: 0.419\n\n9 epoch loss: 0.388\n\n10 epoch loss: 0.372\n\nFinished Training\n\nAccuracy of the network on the 10000 test images: 81 %\n"}]},{"cell_type":"code","source":["# Define transforms for the dataset\n","transform = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize(224),\n","#     torchvision.transforms.Resize((224,224)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load CIFAR-10 dataset\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n","\n","# # Load pre-trained VGG19 model\n","# vgg19 = torchvision.models.vgg19(weights='VGG19_Weights.DEFAULT')\n","# print(vgg19.features)\n","# print(vgg19.features.parameters())\n","# print(vgg19.classifier)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-22T07:58:44.356081Z","iopub.execute_input":"2024-03-22T07:58:44.356898Z","iopub.status.idle":"2024-03-22T07:58:47.804776Z","shell.execute_reply.started":"2024-03-22T07:58:44.356860Z","shell.execute_reply":"2024-03-22T07:58:47.803897Z"},"trusted":true,"id":"TbHzJF4mQsKG","outputId":"18f98c74-2999-4bd8-d24c-dedb5a06560c"},"execution_count":null,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (17): ReLU(inplace=True)\n  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (24): ReLU(inplace=True)\n  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (26): ReLU(inplace=True)\n  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (31): ReLU(inplace=True)\n  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (33): ReLU(inplace=True)\n  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (35): ReLU(inplace=True)\n  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n<generator object Module.parameters at 0x7cb1dacea570>\nSequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":["# Freeze convolutional layers\n","for param in vgg19.features[:29].parameters():\n","    param.requires_grad = False\n","\n","for param in vgg19.features[29:].parameters():\n","    param.requires_grad = True\n","\n","# Modify the last fully connected layer to output 10 classes\n","# num_features = vgg19.classifier[6].in_features\n","# features = list(vgg19.classifier.children())[:-1]\n","# features.extend([nn.Linear(num_features, 10)])\n","# vgg19.classifier = nn.Sequential(*features)\n","vgg19.classifier[6] = nn.Linear(vgg19.classifier[6].in_features, 10)\n","\n","\n","# Transfer model to device\n","vgg19.to(device)\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(vgg19.parameters(), lr=0.001)\n","\n","# Train the model\n","epochs = 5\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","    # for inputs, labels in trainloader:\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = vgg19(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        # if i % 100 == 99:    # print every 100 mini-batches\n","    print('epoch %d loss: %f' % (epoch + 1, running_loss / len(trainloader)))\n","        # running_loss = 0.0\n","\n","# Evaluate the model\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        outputs = vgg19(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710796037167,"user_tz":-330,"elapsed":3003130,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"7aa0dc79-2d9e-4b20-917d-2c0f0b022609","id":"zCyAEkzaH2bI","execution":{"iopub.status.busy":"2024-03-22T07:58:47.806241Z","iopub.execute_input":"2024-03-22T07:58:47.806519Z","iopub.status.idle":"2024-03-22T08:20:54.596271Z","shell.execute_reply.started":"2024-03-22T07:58:47.806493Z","shell.execute_reply":"2024-03-22T08:20:54.595265Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"epoch 1 loss: 0.918857\nepoch 2 loss: 0.596899\nepoch 3 loss: 0.527485\nepoch 4 loss: 0.460002\nepoch 5 loss: 0.451776\nAccuracy of the network on the 10000 test images: 81 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":["Comparing the performance of Convolutional Neural Networks (CNNs) and Multilayer Perceptrons (MLPs) involves considering various aspects such as their architectures, applications, and performance metrics. Here's a critical comparison between CNNs and MLPs:\n","\n","1. **Architecture**:\n","   - CNN: CNNs are designed specifically for processing grid-like data such as images. They consist of convolutional layers followed by pooling layers and fully connected layers. Convolutional layers extract features hierarchically through learned filters, while pooling layers reduce spatial dimensions.\n","   - MLP: MLPs are basic feedforward neural networks where each neuron in one layer is connected to every neuron in the subsequent layer. They consist of an input layer, one or more hidden layers, and an output layer.\n","\n","2. **Feature Learning**:\n","   - CNN: CNNs are adept at learning hierarchical representations of features, making them highly effective for tasks like image classification, object detection, and image segmentation. They automatically learn features at different levels of abstraction.\n","   - MLP: MLPs require features to be pre-engineered or manually extracted before being fed into the network. They are less efficient in learning hierarchical features directly from raw data, which makes them less suitable for tasks like image processing.\n","\n","3. **Parameter Efficiency**:\n","   - CNN: CNNs are parameter efficient due to weight sharing in convolutional layers. Shared weights allow CNNs to learn spatial hierarchies of features with fewer parameters, making them suitable for large-scale image datasets.\n","   - MLP: MLPs require a large number of parameters, especially when dealing with high-dimensional data like images. This can lead to overfitting, especially when the dataset is limited.\n","\n","4. **Performance**:\n","   - CNN: CNNs generally outperform MLPs on tasks involving image data due to their ability to capture spatial hierarchies of features. They achieve state-of-the-art performance in tasks like image classification, object detection, and image segmentation.\n","   - MLP: MLPs perform well on tasks where the input-output mapping is relatively simple and the data is low-dimensional. They are suitable for tasks like tabular data classification and regression.\n","\n","5. **Training Time**:\n","   - CNN: Training CNNs can be computationally intensive, especially on large-scale datasets. However, with advancements in hardware (e.g., GPUs, TPUs) and optimization techniques (e.g., mini-batch training, transfer learning), training times have been significantly reduced.\n","   - MLP: MLPs are generally faster to train compared to CNNs, especially on smaller datasets. However, as the complexity of the network and the size of the dataset increase, training times can become significant.\n","\n","In summary, CNNs and MLPs have different strengths and weaknesses based on their architectures and applications. While CNNs excel in processing grid-like data such as images and achieve state-of-the-art performance in various computer vision tasks, MLPs are more suitable for simpler tasks with low-dimensional data."],"metadata":{"id":"ar3hleKG5IeV"}},{"cell_type":"markdown","source":["Transfer learning refers to the practice of leveraging knowledge gained from training a model on one task and applying it to a different but related task. Here's an analysis of the benefits of transfer learning compared to training MLP and CNN models from scratch:\n","\n","1. **Data Efficiency**:\n","   - Transfer Learning: Transfer learning allows models to leverage knowledge from large, pre-existing datasets. This is particularly beneficial when the target task has limited training data. By transferring knowledge from a pre-trained model, even with a smaller dataset, the model can achieve better generalization and performance.\n","   - MLP/CNN from Scratch: Training MLP or CNN models from scratch typically requires a large amount of labeled data to learn meaningful representations. Without sufficient data, these models may suffer from overfitting or fail to generalize well to new data.\n","\n","2. **Training Time**:\n","   - Transfer Learning: Transfer learning can significantly reduce training time since the pre-trained model has already learned meaningful features from a large dataset. Fine-tuning the pre-trained model on a new task usually requires fewer epochs compared to training from scratch.\n","   - MLP/CNN from Scratch: Training MLP or CNN models from scratch can be time-consuming, especially when dealing with large datasets and complex architectures. It requires training the model from random initialization, which may take longer to converge compared to fine-tuning a pre-trained model.\n","\n","3. **Performance**:\n","   - Transfer Learning: Transfer learning often leads to better performance compared to training from scratch, especially when the pre-trained model is from a domain similar to the target task. By transferring knowledge of learned features, the model can achieve better generalization and accuracy on the target task, even with limited training data.\n","   - MLP/CNN from Scratch: Training MLP or CNN models from scratch may lead to suboptimal performance, especially when dealing with limited training data. These models might struggle to learn meaningful representations without sufficient data, leading to poorer performance compared to transfer learning.\n","\n","4. **Resource Utilization**:\n","   - Transfer Learning: Transfer learning allows for efficient utilization of computational resources by leveraging pre-trained models. Instead of starting from random initialization, resources are focused on fine-tuning the model on the target task, which typically requires fewer computational resources.\n","   - MLP/CNN from Scratch: Training MLP or CNN models from scratch requires significant computational resources, including processing power and memory, especially for large-scale datasets. This may not be feasible in resource-constrained environments.\n","\n","5. **Domain Adaptation**:\n","   - Transfer Learning: Transfer learning facilitates domain adaptation by transferring knowledge learned from one domain to another. It allows models to adapt quickly to new domains or tasks without requiring extensive retraining.\n","   - MLP/CNN from Scratch: Training MLP or CNN models from scratch on a new domain may require substantial retraining and tuning of hyperparameters to achieve comparable performance, which can be time-consuming and resource-intensive.\n","\n","In summary, transfer learning offers several benefits over training MLP or CNN models from scratch, including improved data efficiency, reduced training time, better performance, efficient resource utilization, and facilitation of domain adaptation. It is particularly advantageous when dealing with limited training data or when computational resources are constrained."],"metadata":{"id":"HNlqeUEm5MyY"}},{"cell_type":"markdown","source":["The differences in performance between Convolutional Neural Networks (CNNs) and Multilayer Perceptrons (MLPs) stem from their distinct architectures and how they leverage the spatial structure of images for feature extraction. Here's an explanation of how CNNs utilize the spatial structure of images more effectively compared to MLPs:\n","\n","1. **Local Connectivity and Weight Sharing**:\n","   - CNNs: CNNs exploit the spatial locality of features in images by using convolutional layers. In these layers, each neuron is connected only to a local region of the input image (receptive field) through a set of learnable weights (filters). This local connectivity allows CNNs to capture spatial patterns and features such as edges, textures, and shapes efficiently. Additionally, weight sharing ensures that the same set of filters is applied across the entire input image, enabling the model to learn translation-invariant features.\n","   - MLPs: In contrast, MLPs treat input data as a flattened vector, disregarding spatial information. Each neuron in an MLP's hidden layer is connected to every neuron in the preceding layer, resulting in a fully connected architecture. This lack of locality and weight sharing makes MLPs less effective in capturing spatial structures present in images.\n","\n","2. **Hierarchical Feature Extraction**:\n","   - CNNs: CNN architectures typically consist of multiple convolutional layers followed by pooling layers. Convolutional layers learn hierarchical representations of features by progressively extracting more abstract and complex patterns from the input image. Each layer captures different levels of detail, allowing the network to learn increasingly sophisticated features. Pooling layers further enhance translation invariance and reduce spatial dimensions, making the learned features more robust.\n","   - MLPs: MLPs lack the ability to learn hierarchical representations of features directly from the input image. Since all neurons in each layer are connected, MLPs struggle to capture spatial hierarchies effectively. They require manually engineered features or extensive preprocessing to extract meaningful information from images, limiting their ability to generalize well to complex visual tasks.\n","\n","3. **Parameter Efficiency**:\n","   - CNNs: CNNs are parameter efficient due to weight sharing and sparse connectivity. By sharing weights across different spatial locations, CNNs require fewer parameters compared to MLPs, making them better suited for processing high-dimensional data like images. This parameter efficiency allows CNNs to generalize well even with limited training data.\n","   - MLPs: MLPs typically have a large number of parameters, especially when dealing with high-dimensional input data like images. The fully connected nature of MLPs results in a dense parameter matrix, leading to a higher risk of overfitting, especially when trained on limited datasets.\n","\n","In summary, CNNs leverage the spatial structure of images more effectively compared to MLPs through local connectivity, weight sharing, and hierarchical feature extraction. These architectural characteristics enable CNNs to capture spatial patterns and hierarchies of features efficiently, leading to superior performance in various computer vision tasks such as image classification, object detection, and image segmentation."],"metadata":{"id":"YCIutrct5OjN"}},{"cell_type":"markdown","source":["The VGG (Visual Geometry Group) model is a widely-used convolutional neural network architecture that was introduced by the Visual Geometry Group at the University of Oxford. It is known for its simplicity and effectiveness in image classification tasks. When applying transfer learning with the VGG model, several aspects contribute to the improvement in performance:\n","\n","1. **Pre-trained Feature Extraction**:\n","   - The VGG model, pre-trained on large-scale image datasets such as ImageNet, has learned to extract hierarchical features from images. These features include simple shapes, textures, and complex patterns.\n","   - By leveraging the pre-trained VGG model's feature extraction capabilities, transfer learning allows us to utilize these learned representations for a new, potentially different task.\n","   - This feature extraction capability is crucial, especially when dealing with tasks where obtaining a large labeled dataset for training from scratch is impractical.\n","\n","2. **Fine-tuning**:\n","   - Transfer learning often involves fine-tuning the pre-trained VGG model on a new dataset related to the target task. Fine-tuning entails adjusting the model's parameters (weights) during training on the new dataset while retaining the learned representations from the pre-trained model.\n","   - Fine-tuning allows the model to adapt its learned features to the specific characteristics of the new dataset, thereby improving its performance on the target task.\n","   - The number of layers that are fine-tuned and the learning rate during fine-tuning are hyperparameters that can be adjusted to balance between retaining the generalization of the pre-trained model and adapting to the new task.\n","\n","3. **Reduced Training Time and Data Requirements**:\n","   - Training a deep neural network from scratch, especially one as deep as the VGG model, requires a significant amount of computational resources and time.\n","   - Transfer learning with the pre-trained VGG model significantly reduces both the time and data requirements for training, as the model starts with learned features that are already useful for the task at hand.\n","   - This reduction in training time and data requirements is particularly advantageous when working with limited computational resources or when only a small labeled dataset is available.\n","\n","4. **Generalization**:\n","   - The pre-trained VGG model has learned to extract features that are generally useful across a wide range of visual recognition tasks.\n","   - By leveraging this pre-trained model, transfer learning helps improve the generalization of the model to the target task. It allows the model to benefit from the knowledge learned from a diverse set of images during pre-training.\n","   - This improved generalization often leads to better performance, especially when the target task shares similarities with the tasks encountered during pre-training.\n","\n","In summary, transfer learning with the VGG model improves performance by leveraging pre-trained feature extraction capabilities, fine-tuning the model on the new dataset, reducing training time and data requirements, and enhancing generalization to the target task. These benefits make transfer learning with the VGG model an effective approach for various computer vision tasks, including image classification, object detection, and image segmentation."],"metadata":{"id":"v2S7_uOY5Qj4"}}]}