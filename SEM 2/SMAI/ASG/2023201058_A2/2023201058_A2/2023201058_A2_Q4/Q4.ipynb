{"cells":[{"cell_type":"markdown","metadata":{"id":"lvpf93Fv9Hlk"},"source":["# SMAI Assignment - 2\n","\n","## Question 4: Multi-layer Perceptrons\n","\n","### Digit Classification\n","\n","In this question, you will perform digit classification using MLP. You can use the MLPClassifier from sklearn. Train and two test sets have been provided [here](https://drive.google.com/drive/folders/1OUVrOMp2jSSBDJSqvEyXDFTrhiyZnqit?usp=sharing). Report the accuracy and any other interesting observations."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"G-MpagLl-YJp","executionInfo":{"status":"ok","timestamp":1709310517783,"user_tz":-330,"elapsed":4,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"outputs":[],"source":["import numpy as np\n","import h5py\n","# from collections import Counter\n","from sklearn.neural_network import MLPClassifier\n","# import matplotlib.pyplot as plt\n","# from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report"]},{"cell_type":"code","source":["# train = h5py.File(\"training_3digits.hdf5\",'r')\n","# test1 = h5py.File(\"testing_3digits_part1.hdf5\",'r')\n","# test2 = h5py.File(\"testing_3digits_part2.hdf5\",'r')\n","# !gdown 1RjT1-Gdnrtx8sy7MKccW-CS4uLgVpCz3\n","# !gdown 10UKA1CxjanTua_t3lYr476t6rjjelR_V\n","# !gdown 1-G0POq4nyCLweM27kL02FAo6GfknD2JY\n","!gdown 1mEFOix1IiH6GgrQEa09pb13yztzxhw9q\n","!gdown 1qLcSIyoWHMZPOYqifjdFhL4tPe3dreAp\n","!gdown 1UTuM9fBkJ_eRF5BvYp9Hj0KNQ6L35g2t\n","# train = h5py.File(\"/content/training_3digits.hdf5\",'r')\n","# test1 = h5py.File(\"/content/testing_3digits_part1.hdf5\",'r')\n","# test2 = h5py.File(\"/content/testing_3digits_part2.hdf5\",'r')\n","with h5py.File(\"training_3digits.hdf5\",'r') as train:\n","    train_images = np.array(train['images'])\n","    train_digits = np.array(train['digits'])\n","    # print(train_images.shape)\n","    # print(train_digits.shape)\n","    # train.close()\n","    # # gray_train_images = np.dot(train_images[...,:3], [0.2989, 0.5870, 0.1140])\n","    # gray_train_images = np.all(train_images, axis=-1)\n","    # # print(gray_train_images.shape)\n","    # train_digits_dict = dict(Counter(train_digits))\n","    # print(train_digits_dict)\n","with h5py.File(\"testing_3digits_part1.hdf5\",'r') as test1:\n","    test_images_1 = np.array(test1['images'])\n","    test_digits_1 = np.array(test1['digits'])\n","    # print(test_images_1.shape)\n","    # print(test_digits_1.shape)\n","    # test1.close()\n","    # # gray_test_images_1 = np.dot(test_images_1[...,:3], [0.2989, 0.5870, 0.1140])\n","    # gray_test_images_1 = np.all(test_images_1, axis=-1)\n","    # # print(gray_test_images_1.shape)\n","    # test_digits_1_dict = dict(Counter(test_digits_1))\n","    # print(test_digits_1_dict)\n","with h5py.File(\"testing_3digits_part2.hdf5\",'r') as test2:\n","    test_images_2 = np.array(test2['images'])\n","    test_digits_2 = np.array(test2['digits'])\n","    # print(test_images_2.shape)\n","    # print(test_digits_2.shape)\n","    # test2.close()\n","    # # gray_test_images_2 = np.dot(1-test_images_2[...,:3], [0.2989, 0.5870, 0.1140])\n","    # gray_test_images_2 = 1-np.all(test_images_2, axis=-1)\n","    # # print(gray_test_images_2.shape)\n","    # test_digits_2_dict = dict(Counter(test_digits_2))\n","    # print(test_digits_2_dict)"],"metadata":{"id":"mV08nXEXsWp0","executionInfo":{"status":"ok","timestamp":1709310526285,"user_tz":-330,"elapsed":8092,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"71f1e0f1-2666-44da-fc72-7af4e2fa9e30"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1mEFOix1IiH6GgrQEa09pb13yztzxhw9q\n","To: /content/testing_3digits_part1.hdf5\n","100% 29.6M/29.6M [00:00<00:00, 110MB/s] \n","Downloading...\n","From: https://drive.google.com/uc?id=1qLcSIyoWHMZPOYqifjdFhL4tPe3dreAp\n","To: /content/testing_3digits_part2.hdf5\n","100% 29.6M/29.6M [00:00<00:00, 113MB/s] \n","Downloading...\n","From: https://drive.google.com/uc?id=1UTuM9fBkJ_eRF5BvYp9Hj0KNQ6L35g2t\n","To: /content/training_3digits.hdf5\n","100% 25.7M/25.7M [00:00<00:00, 109MB/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YK24sPMEsZMQ","executionInfo":{"status":"ok","timestamp":1709310526285,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"24bxfhzkskf9","executionInfo":{"status":"ok","timestamp":1709310526972,"user_tz":-330,"elapsed":10,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"id":"cHAbz92352gz","executionInfo":{"status":"ok","timestamp":1709310526974,"user_tz":-330,"elapsed":11,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"5MLBdrs65woF","executionInfo":{"status":"ok","timestamp":1709310526974,"user_tz":-330,"elapsed":10,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"outputs":[],"source":["\n","# # # Visualize a few sample images\n","\n","\n","# # # Generate 5 random indices\n","# # random_indices = [572,577,580,581,585,1111,1112,1115,1122,1125,2472,2481,2482,2485,2487]\n","# random_indices = np.random.randint(0, len(train_images), size=15)\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(train_images[idx])\n","#     plt.title(f\"Digit: {train_digits[idx]}\")\n","#     plt.axis('off')\n","\n","# # Generate 5 random indices\n","# random_indices = np.random.randint(0, len(test_images_1), size=15)\n","# # random_indices = [1000, 2000, 3000]\n","\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(test_images_1[idx])\n","#     plt.title(f\"Digit: {test_digits_1[idx]}\")\n","#     plt.axis('off')\n","\n","# # Generate 5 random indices\n","# random_indices = np.random.randint(0, len(test_images_2), size=15)\n","# array_3d = np.ones((28, 28, 3))\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(test_images_2[idx])\n","#     plt.title(f\"Digit: {test_digits_2[idx]}\")\n","#     plt.axis('off')\n","#     # np.where(image == color1, color2, np.where(image == color2, color1, image))\n","\n","# # Generate 5 random indices\n","# random_indices = np.random.randint(0, len(gray_test_images_1), size=15)\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(gray_test_images_1[idx])\n","#     plt.title(f\"Digit: {test_digits_1[idx]}\")\n","#     plt.axis('off')\n","\n","# # Generate 5 random indices\n","# random_indices = np.random.randint(0, len(gray_test_images_2), size=15)\n","# # array_3d = np.ones((28, 28, 3))\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(gray_test_images_2[idx])\n","#     plt.title(f\"Digit: {test_digits_2[idx]}\")\n","#     plt.axis('off')\n","\n","# # print(gray_test_images_1[0], gray_test_images_2[0])\n","# # for i in [572, 1111, 2472]:\n","#     # print(train_images[i][0][0])\n","#     # print(test_images_1[i][0][0])\n","#     # print(test_images_2[i][0][0])"]},{"cell_type":"code","source":["# plt.imshow(np.dot(train_images[1][..., :3], [0.2989, 0.5870, 0.1140]))"],"metadata":{"id":"1Bugr9Bz0ckL","executionInfo":{"status":"ok","timestamp":1709310526974,"user_tz":-330,"elapsed":10,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# plt.imshow(np.dot(test_images_1[2][..., :3], [0.2989, 0.5870, 0.1140]))\n","# plt.imshow(test_images_1[2])\n","# print(test_images_1[2][14])"],"metadata":{"id":"mWQz27_o8Glz","executionInfo":{"status":"ok","timestamp":1709310526974,"user_tz":-330,"elapsed":9,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# plt.imshow(np.dot(1-test_images_2[2][..., :3], [0.2989, 0.5870, 0.1140]))\n","# plt.imshow(1-test_images_2[2])\n","# print(test_images_2[2][14])"],"metadata":{"id":"vJG5Ykn88Goo","executionInfo":{"status":"ok","timestamp":1709310526975,"user_tz":-330,"elapsed":10,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# print(set(train_digits))\n","# print(set(test_digits_1))\n","# print(set(test_digits_2))"],"metadata":{"id":"_66JtlCTz9Df","executionInfo":{"status":"ok","timestamp":1709310526975,"user_tz":-330,"elapsed":9,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["\n","# # Visualize a few sample images\n","# # Generate 5 random indices\n","# random_indices = np.random.randint(0, len(gray_train_images), size=15)\n","\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(gray_train_images[idx])\n","#     plt.title(f\"Digit: {train_digits[idx]}\")\n","#     plt.axis('off')\n","\n","# # Generate 5 random indices\n","# random_indices = np.random.randint(0, len(gray_test_images_1), size=15)\n","\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(gray_test_images_1[idx])\n","#     plt.title(f\"Digit: {test_digits_1[idx]}\")\n","#     plt.axis('off')\n","\n","# # Generate 5 random indices\n","# random_indices = np.random.randint(0, len(gray_test_images_2), size=15)\n","# # array_3d = np.ones((28, 28, 3))\n","# # Visualize the random images\n","# plt.figure(figsize=(15, 15))\n","# for i, idx in enumerate(random_indices):\n","#     plt.subplot(1, 15, i + 1)\n","#     plt.imshow(gray_test_images_2[idx])\n","#     plt.title(f\"Digit: {test_digits_2[idx]}\")\n","#     plt.axis('off')"],"metadata":{"id":"N25hWBPvv5ZZ","executionInfo":{"status":"ok","timestamp":1709310526975,"user_tz":-330,"elapsed":8,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Flatten the images\n","# gray_train_images_flat = gray_train_images.reshape(gray_train_images.shape[0], -1)\n","# gray_test_images_1_flat = gray_test_images_1.reshape(gray_test_images_1.shape[0], -1)\n","# gray_test_images_2_flat = gray_test_images_2.reshape(gray_test_images_2.shape[0], -1)\n","\n","train_images_flat = train_images.reshape(train_images.shape[0], -1)\n","test_images_1_flat = test_images_1.reshape(test_images_1.shape[0], -1)\n","test_images_2_flat = test_images_2.reshape(test_images_2.shape[0], -1)\n","\n","scaler = StandardScaler()\n","# gray_train_images_flat = scaler.fit_transform(gray_train_images_flat)\n","# gray_test_images_1_flat = scaler.fit_transform(gray_test_images_1_flat)\n","# gray_test_images_2_flat = scaler.fit_transform(gray_test_images_2_flat)\n","\n","train_images_flat = scaler.fit_transform(train_images_flat)\n","test_images_1_flat = scaler.transform(test_images_1_flat)\n","test_images_2_flat = scaler.transform(test_images_2_flat)"],"metadata":{"id":"8k-aJmbZqZYX","executionInfo":{"status":"ok","timestamp":1709310526976,"user_tz":-330,"elapsed":9,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Hyperparameter tuning using GridSearchCV\n","# param_grid = {\n","#     'hidden_layer_sizes': [(50,), (100,), (200,), (50, 50), (100, 50)],\n","#     'alpha': [0.0001, 0.001, 0.01, 0.1],\n","#     'learning_rate_init': [0.0001, 0.001, 0.01, 0.1],\n","#     'activation': ['relu', 'tanh']\n","# }\n","# param_grid = {\n","#     'hidden_layer_sizes': [(120,80,40)],\n","# }\n","param_grid = {\n","    'hidden_layer_sizes': [(100), (120,80,40)],\n","    'max_iter': 500\n","}\n","# Best hyperparameters: {'activation': 'relu', 'hidden_layer_sizes': (120, 80, 40), 'max_iter': 300}\n","# Initialize and train the MLPClassifier\n","mlp = MLPClassifier(hidden_layer_sizes = (100), solver='sgd')\n","grid_search = GridSearchCV(mlp, param_grid, verbose=5)\n","\n","# # Perform cross-validation on the training data\n","# kf = KFold(n_splits=10, shuffle=True, random_state=42)\n","# cv_scores = cross_val_score(mlp, train_images_flat, train_digits, cv=kf, scoring='accuracy')\n","\n","# print(\"Cross-Validation Scores:\", cv_scores)\n","# print(\"Mean Accuracy:\", np.mean(cv_scores))\n","\n","# mlp.fit(train_images_flat, train_digits)\n","\n","# # Evaluate on the first test set\n","# predictions_1 = mlp.predict(test_images_1_flat)\n","# accuracy_1 = accuracy_score(test_digits_1, predictions_1)\n","# print(\"Accuracy on test set 1:\", accuracy_1)\n","\n","# # Evaluate on the second test set\n","# predictions_2 = mlp.predict(test_images_2_flat)\n","# accuracy_2 = accuracy_score(test_digits_2, predictions_2)\n","# print(\"Accuracy on test set 2:\", accuracy_2)\n","\n","\n","# train_images_flat1, _, train_digits1, _ = train_test_split(train_images_flat, train_digits, test_size=0.000000001, random_state=42)\n","mlp.fit(train_images_flat, train_digits)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":92},"id":"GU7dPuiRfwHW","executionInfo":{"status":"ok","timestamp":1709310532199,"user_tz":-330,"elapsed":5231,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"b94fc706-d05b-4deb-b4fb-6743a7ce4055"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=100, learning_rate_init=0.1, max_iter=500,\n","              random_state=1, solver='sgd')"],"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=100, learning_rate_init=0.1, max_iter=500,\n","              random_state=1, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=100, learning_rate_init=0.1, max_iter=500,\n","              random_state=1, solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# Print the best hyperparameters\n","print(\"Best hyperparameters:\", grid_search.best_params_)\n","\n","# Evaluate on the first test set using the best model\n","# best_model = grid_search\n","predictions_1 = mlp.predict(test_images_1_flat)\n","accuracy_1 = accuracy_score(test_digits_1, predictions_1)\n","print(f\"Accuracy on test set 1: {accuracy_1 * 100:.2f}%\")\n","\n","# Evaluate on the second test set using the best model\n","predictions_2 = mlp.predict(test_images_2_flat)\n","accuracy_2 = accuracy_score(test_digits_2, predictions_2)\n","print(f\"Accuracy on test set 2: {accuracy_2 * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grjX--q8ukHR","executionInfo":{"status":"ok","timestamp":1709310532200,"user_tz":-330,"elapsed":11,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"6ad48be0-888d-4ab5-9b30-e4dcb40d9eca"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test set 1: 99.97%\n","Accuracy on test set 2: 32.76%\n"]}]},{"cell_type":"code","source":["for i in test_digits_2:\n","    print(i, end=', ')\n","print()\n","for i in predictions_2:\n","    print(i, end=', ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzKmO4NTCI71","executionInfo":{"status":"ok","timestamp":1709310534137,"user_tz":-330,"elapsed":1945,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}},"outputId":"94d88bc6-0931-40a1-bfe2-ee29fdfc8e39"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n","2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, "]}]},{"cell_type":"markdown","source":["#Observations\n","\n"],"metadata":{"id":"iZzKhHbGimDD"}},{"cell_type":"markdown","source":["From images plotted above, it's observed that test set 1 is a subset of the training set (hence higher accuracy) and both sets contain numbers written in RGB colors on white background. Set 2 is the opposite where white numbers are written on colored background which isn't a part of training set (hence low accuracy). It also seems the network is learning colors by mapping it to numbers instead of numbers themselves."],"metadata":{"id":"zvrBcK3xG0Ti"}},{"cell_type":"code","source":["# # Hyperparameter tuning using GridSearchCV\n","# param_grid = {\n","#     'hidden_layer_sizes': [(250, )],\n","#     'activation': ['relu', 'tanh']\n","# }\n","\n","# # Initialize and train the MLPClassifier\n","# mlp = MLPClassifier(solver='sgd') #, max_iter=1000\n","# grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', verbose=5)\n","\n","# grid_search.fit(train_images_flat, train_digits)\n","\n","# # Print the best hyperparameters\n","# print(\"Best hyperparameters:\", grid_search.best_params_)\n","\n","# # Evaluate on the first test set using the best model\n","# best_model = grid_search\n","# predictions_1 = best_model.predict(test_images_1_flat)\n","# accuracy_1 = accuracy_score(test_digits_1, predictions_1)\n","# print(\"Accuracy on test set 1:\", accuracy_1)\n","\n","# # Evaluate on the second test set using the best model\n","# predictions_2 = best_model.predict(test_images_2_flat)\n","# accuracy_2 = accuracy_score(test_digits_2, predictions_2)\n","# print(\"Accuracy on test set 2:\", accuracy_2)"],"metadata":{"id":"VjLNnGADlvoG","executionInfo":{"status":"ok","timestamp":1709310534138,"user_tz":-330,"elapsed":5,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# # Hyperparameter tuning using GridSearchCV\n","# param_grid = {\n","#     'hidden_layer_sizes': [(50,), (100,), (200,), (50, 50), (100, 50)],\n","#     'alpha': [0.0001, 0.001, 0.01, 0.1],\n","#     'learning_rate_init': [0.0001, 0.001, 0.01, 0.1],\n","#     'activation': ['relu', 'tanh']\n","# }\n","\n","# # Initialize and train the MLPClassifier\n","# mlp = MLPClassifier(solver='sgd') #, max_iter=1000\n","# grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', verbose=5)\n","\n","# # # Perform cross-validation on the training data\n","# # kf = KFold(n_splits=10, shuffle=True, random_state=42)\n","# # cv_scores = cross_val_score(mlp, train_images_flat, train_digits, cv=kf, scoring='accuracy')\n","\n","# # print(\"Cross-Validation Scores:\", cv_scores)\n","# # print(\"Mean Accuracy:\", np.mean(cv_scores))\n","\n","# # mlp.fit(train_images_flat, train_digits)\n","\n","# # # Evaluate on the first test set\n","# # predictions_1 = mlp.predict(test_images_1_flat)\n","# # accuracy_1 = accuracy_score(test_digits_1, predictions_1)\n","# # print(\"Accuracy on test set 1:\", accuracy_1)\n","\n","# # # Evaluate on the second test set\n","# # predictions_2 = mlp.predict(test_images_2_flat)\n","# # accuracy_2 = accuracy_score(test_digits_2, predictions_2)\n","# # print(\"Accuracy on test set 2:\", accuracy_2)\n","\n","\n","\n","# grid_search.fit(train_images_flat, train_digits)\n","\n","# # Print the best hyperparameters\n","# print(\"Best hyperparameters:\", grid_search.best_params_)\n","# # Best hyperparameters: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (300,), 'learning_rate_init': 0.01}\n","\n","# # Evaluate on the first test set using the best model\n","# best_model = grid_search\n","# predictions_1 = best_model.predict(test_images_1_flat)\n","# accuracy_1 = accuracy_score(test_digits_1, predictions_1)\n","# print(\"Accuracy on test set 1:\", accuracy_1)\n","\n","# # Evaluate on the second test set using the best model\n","# predictions_2 = best_model.predict(test_images_2_flat)\n","# accuracy_2 = accuracy_score(test_digits_2, predictions_2)\n","# print(\"Accuracy on test set 2:\", accuracy_2)"],"metadata":{"id":"ut9SqdwqEkqW","executionInfo":{"status":"ok","timestamp":1709310534138,"user_tz":-330,"elapsed":4,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# # Hyperparameter tuning using GridSearchCV\n","# # param_grid = {\n","# #     'hidden_layer_sizes': [(50,), (100,), (200,), (50, 50), (100, 50)],\n","# #     'alpha': [0.0001, 0.001, 0.01, 0.1],\n","# #     'learning_rate_init': [0.0001, 0.001, 0.01, 0.1],\n","# #     'activation': ['relu', 'tanh']\n","# # }\n","# # param_grid = {\n","# #     'hidden_layer_sizes': [(120,80,40)],\n","# # }\n","# param_grid = {\n","#     'hidden_layer_sizes': [(9, 9)],\n","# }\n","# # Best hyperparameters: {'activation': 'relu', 'hidden_layer_sizes': (120, 80, 40), 'max_iter': 300}\n","# # Initialize and train the MLPClassifier\n","# mlp = MLPClassifier(solver='sgd')\n","# grid_search = GridSearchCV(mlp, param_grid, scoring='accuracy', verbose=5)\n","\n","# # # Perform cross-validation on the training data\n","# # kf = KFold(n_splits=10, shuffle=True, random_state=42)\n","# # cv_scores = cross_val_score(mlp, train_images_flat, train_digits, cv=kf, scoring='accuracy')\n","\n","# # print(\"Cross-Validation Scores:\", cv_scores)\n","# # print(\"Mean Accuracy:\", np.mean(cv_scores))\n","\n","# # mlp.fit(train_images_flat, train_digits)\n","\n","# # # Evaluate on the first test set\n","# # predictions_1 = mlp.predict(test_images_1_flat)\n","# # accuracy_1 = accuracy_score(test_digits_1, predictions_1)\n","# # print(\"Accuracy on test set 1:\", accuracy_1)\n","\n","# # # Evaluate on the second test set\n","# # predictions_2 = mlp.predict(test_images_2_flat)\n","# # accuracy_2 = accuracy_score(test_digits_2, predictions_2)\n","# # print(\"Accuracy on test set 2:\", accuracy_2)\n","\n","\n","# # gray_train_images_flat, _, train_digits, _ = train_test_split(gray_train_images_flat, train_digits, test_size=0.000001, random_state=42)\n","# grid_search.fit(gray_train_images_flat, train_digits)\n","\n","# # Print the best hyperparameters\n","# print(\"Best hyperparameters:\", grid_search.best_params_)\n","\n","# # Evaluate on the first test set using the best model\n","# best_model = grid_search\n","# predictions_1 = best_model.predict(gray_test_images_1_flat)\n","# accuracy_1 = accuracy_score(test_digits_1, predictions_1)\n","# print(\"Accuracy on test set 1:\", accuracy_1)\n","\n","# # Evaluate on the second test set using the best model\n","# predictions_2 = best_model.predict(gray_test_images_2_flat)\n","# accuracy_2 = accuracy_score(test_digits_2, predictions_2)\n","# print(\"Accuracy on test set 2:\", accuracy_2)"],"metadata":{"id":"I_Jr0WzvCK_V","executionInfo":{"status":"ok","timestamp":1709310534139,"user_tz":-330,"elapsed":4,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# for i in test_digits_2:\n","#     print(i, end=', ')\n","# print()\n","# for i in predictions_2:\n","#     print(i, end=', ')"],"metadata":{"id":"PmSogG4Vf9Ro","executionInfo":{"status":"ok","timestamp":1709310534793,"user_tz":-330,"elapsed":658,"user":{"displayName":"Hemanth Reddy","userId":"09263723995373073861"}}},"execution_count":32,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}